{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJnwpdtXTuF7"
      },
      "source": [
        "# Decision Transformer\n",
        "\n",
        "From blogpost https://sites.google.com/berkeley.edu/decision-transformer\n",
        "\n",
        "### Offline reinforcement learning as a sequence modeling problem\n",
        "\n",
        "We investigate shifting our perspective of reinforcement learning (RL) by posing sequential decision making problems in a language modeling framework. While conventional work in RL has utilized specialized frameworks relying on Bellman backups, we propose to instead model trajectories with sequence modeling, enabling us to use strong and well-studied architectures such as transformers to generate behaviors. To illustrate this, we study offline reinforcement learning, where we train a model from a fixed dataset rather than collecting experience in the environment. This enables us to train RL policies using the same code as a language modeling framework, with minimal changes.\n",
        "\n",
        "### Decision Transformer: autoregressive sequence modeling for RL\n",
        "\n",
        "We take a simple approach: each modality (return, state, or action) is passed into an embedding network (convolutional encoder for images, linear layer for continuous states). The embeddings are then processed by an autoregressive transformer model, trained to predict the next action given the previous tokens using a linear output layer.\n",
        "\n",
        "Evaluation is also easy: we can initialize by a desired target return (e.g. 1 or 0 for success or failure) and the starting state in the environment. Unrolling the sequence -- similar to standard autoregressive generation in language models -- yields a sequence of actions to execute in the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://lh3.googleusercontent.com/bSBvjrX-u5eBAjo0zwOF4OOPqueYw7XCU3rfcJkusXwbO8q7N5LPxvNxuerYCWuosKe9juzxkFKEhurF1kLDnEv41zH2c46m-XDDW0TjomU0Xh6Mi4eaxbwW4AFvR65rIQ=w1280\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stitching subsequences to produce optimal trajectories\n",
        "\n",
        "<img src=\"https://lh3.googleusercontent.com/VHP48-e0T7qNnqLpx_Hve46HHIF0nOKflIcTzTYL6bthDBzu0rJZKcb0XbZowDOkiYaiVtYBejwWiEcCFDqmCEMPHTQ3bpLx2q6fPSPJEYUvy0_S6mNGyObHcYXCPUAN8w=w1280\" />\n",
        "\n",
        "Consider the task of finding the shortest path on a fixed graph, posed as a reinforcement learning problem (accumulated reward = sum of edge weights). In a training dataset consisting of random walks, we observe many suboptimal trajectories. If we train Decision Transformer on these sequences, we can ask the model to generate an optimal path by conditioning on a large return. We find that by training on only random walks, Decision Transformer can learn to stitch together subsequences from different training trajectories in order to produce optimal trajectories at test time!\n",
        "\n",
        "In fact, this is the same behavior which is desired from off-policy Q-learning algorithms commonly used in offline reinforcement learning frameworks. However, without needing to introduce TD learning algorithms, value pessimism, or behavior regularization , we can achieve the same behavior using a sequence modeling framework!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I30mBweWvTz6"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/edbeeching/decision-transformer-gym-halfcheetah-expert/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCsT6KVxwARY"
      },
      "source": [
        "### Step 1: Install dependencies for model evaluation üîΩ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qntpWiEWX89"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common \\\n",
        "    patchelf \\\n",
        "    xvfb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr-1h83zVsMK"
      },
      "source": [
        "### Step 2: Install and import the packages üì¶\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuXIPJUWVrhq"
      },
      "outputs": [],
      "source": [
        "!pip install gym==0.21.0\n",
        "!pip install free-mujoco-py\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install imageio-ffmpeg\n",
        "\n",
        "!pip install colabgymrender==1.0.2\n",
        "!pip install xvfbwrapper\n",
        "!pip install imageio==2.4.1\n",
        "!pip install imageio-ffmpeg\n",
        "!pip install huggingface_hub\n",
        "!pip install -U accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DktITQNXTopc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import DecisionTransformerConfig, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1Ugq2POUmRA"
      },
      "source": [
        "### Step 3: Loading the dataset from the ü§ó Hub and instantiating the model\n",
        "\n",
        "We host a number of Offline RL Datasets on the hub. Today we will be training with the halfcheetah ‚Äúexpert‚Äù dataset, hosted here on hub.\n",
        "\n",
        "First we need to import the load_dataset function from the ü§ó datasets package and download the dataset to our machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3bLeIHqUwq7"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\" # we diable weights and biases logging for this tutorial\n",
        "dataset = load_dataset(\"edbeeching/decision_transformer_gym_replay\", \"halfcheetah-expert-v2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFmTdHoHUD13"
      },
      "source": [
        "### Step 4: Defining a custom DataCollator for the transformers Trainer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1QzZHmPUM4p"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DecisionTransformerGymDataCollator:\n",
        "    return_tensors: str = \"pt\"\n",
        "    max_len: int = 20 #subsets of the episode we use for training\n",
        "    state_dim: int = 17  # size of state space\n",
        "    act_dim: int = 6  # size of action space\n",
        "    max_ep_len: int = 1000 # max episode length in the dataset\n",
        "    scale: float = 1000.0  # normalization of rewards/returns\n",
        "    state_mean: np.array = None  # to store state means\n",
        "    state_std: np.array = None  # to store state stds\n",
        "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
        "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
        "\n",
        "    def __init__(self, dataset) -> None:\n",
        "        self.act_dim = len(dataset[0][\"actions\"][0])\n",
        "        self.state_dim = len(dataset[0][\"observations\"][0])\n",
        "        self.dataset = dataset\n",
        "        # calculate dataset stats for normalization of states\n",
        "        states = []\n",
        "        traj_lens = []\n",
        "        for obs in dataset[\"observations\"]:\n",
        "            states.extend(obs)\n",
        "            traj_lens.append(len(obs))\n",
        "        self.n_traj = len(traj_lens)\n",
        "        states = np.vstack(states)\n",
        "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
        "\n",
        "        traj_lens = np.array(traj_lens)\n",
        "        self.p_sample = traj_lens / sum(traj_lens)\n",
        "\n",
        "    def _discount_cumsum(self, x, gamma):\n",
        "        discount_cumsum = np.zeros_like(x)\n",
        "        discount_cumsum[-1] = x[-1]\n",
        "        for t in reversed(range(x.shape[0] - 1)):\n",
        "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
        "        return discount_cumsum\n",
        "\n",
        "    def __call__(self, features):\n",
        "        batch_size = len(features)\n",
        "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
        "        batch_inds = np.random.choice(\n",
        "            np.arange(self.n_traj),\n",
        "            size=batch_size,\n",
        "            replace=True,\n",
        "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
        "        )\n",
        "        # a batch of dataset features\n",
        "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
        "\n",
        "        for ind in batch_inds:\n",
        "            # for feature in features:\n",
        "            feature = self.dataset[int(ind)]\n",
        "            si = random.randint(0, len(feature[\"rewards\"]) - 1)\n",
        "\n",
        "            # get sequences from dataset\n",
        "            s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
        "            a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
        "            r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
        "\n",
        "            d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
        "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
        "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
        "            rtg.append(\n",
        "                self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
        "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
        "                ].reshape(1, -1, 1)\n",
        "            )\n",
        "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
        "                print(\"if true\")\n",
        "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
        "\n",
        "            # padding and state + reward normalization\n",
        "            tlen = s[-1].shape[1]\n",
        "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
        "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
        "            a[-1] = np.concatenate(\n",
        "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
        "                axis=1,\n",
        "            )\n",
        "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
        "            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
        "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
        "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
        "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
        "\n",
        "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
        "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
        "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
        "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
        "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
        "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
        "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
        "\n",
        "        return {\n",
        "            \"states\": s,\n",
        "            \"actions\": a,\n",
        "            \"rewards\": r,\n",
        "            \"returns_to_go\": rtg,\n",
        "            \"timesteps\": timesteps,\n",
        "            \"attention_mask\": mask,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Implement Decition Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional, Union, Tuple\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers.models.decision_transformer.modeling_decision_transformer import DecisionTransformerPreTrainedModel, DecisionTransformerOutput, DecisionTransformerGPT2Model\n",
        "\n",
        "\n",
        "class DecisionTransformerModel(DecisionTransformerPreTrainedModel):\n",
        "    \"\"\"\n",
        "\n",
        "    The model builds upon the GPT2 architecture to perform autoregressive prediction of actions in an offline RL\n",
        "    setting. Refer to the paper for more details: https://arxiv.org/abs/2106.01345\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "        self.hidden_size = config.hidden_size\n",
        "        # note: the only difference between this GPT2Model and the default Huggingface version\n",
        "        # is that the positional embeddings are removed (since we'll add those ourselves)\n",
        "        self.encoder = DecisionTransformerGPT2Model(config)\n",
        "\n",
        "        self.embed_timestep = nn.Embedding(config.max_ep_len, config.hidden_size)\n",
        "        self.embed_return = torch.nn.Linear(1, config.hidden_size)\n",
        "        self.embed_state = torch.nn.Linear(config.state_dim, config.hidden_size)\n",
        "        self.embed_action = torch.nn.Linear(config.act_dim, config.hidden_size)\n",
        "\n",
        "        self.embed_ln = nn.LayerNorm(config.hidden_size)\n",
        "\n",
        "        # note: we don't predict states or returns for the paper\n",
        "        self.predict_state = torch.nn.Linear(config.hidden_size, config.state_dim)\n",
        "        self.predict_action = nn.Sequential(\n",
        "            *([nn.Linear(config.hidden_size, config.act_dim)] + ([nn.Tanh()] if config.action_tanh else []))\n",
        "        )\n",
        "        self.predict_return = torch.nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        states: Optional[torch.FloatTensor] = None,\n",
        "        actions: Optional[torch.FloatTensor] = None,\n",
        "        rewards: Optional[torch.FloatTensor] = None,\n",
        "        returns_to_go: Optional[torch.FloatTensor] = None,\n",
        "        timesteps: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.FloatTensor], DecisionTransformerOutput]:\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        batch_size, seq_length = states.shape[0], states.shape[1]\n",
        "\n",
        "        if attention_mask is None:\n",
        "            # attention mask for GPT: 1 if can be attended to, 0 if not\n",
        "            attention_mask = torch.ones((batch_size, seq_length), dtype=torch.long)\n",
        "\n",
        "        # embed each modality with a different head\n",
        "        state_embeddings = self.embed_state(states)\n",
        "        action_embeddings = self.embed_action(actions)\n",
        "        returns_embeddings = self.embed_return(returns_to_go)\n",
        "        time_embeddings = self.embed_timestep(timesteps)\n",
        "\n",
        "        # TODO: add time embeddings to all other embeddings\n",
        "        # time embeddings are treated similar to positional embeddings\n",
        "        state_embeddings = <TODO>\n",
        "        action_embeddings = <TODO>\n",
        "        returns_embeddings = <TODO>\n",
        "\n",
        "        # TODO: stack return, state and action embeddings and permute them to make\n",
        "        # the sequence look like (R_1, s_1, a_1, R_2, s_2, a_2, ...)\n",
        "        # which works nice in an autoregressive sense since states predict actions\n",
        "        stacked_inputs = <TODO>\n",
        "        stacked_inputs = stacked_inputs.reshape(batch_size, 3 * seq_length, self.hidden_size)\n",
        "        stacked_inputs = self.embed_ln(stacked_inputs)\n",
        "\n",
        "        # TODO: do the same with attention mask \n",
        "        # to fit the stacked inputs, have to stack it as well\n",
        "        stacked_attention_mask = <TODO>\n",
        "        stacked_attention_mask = stacked_attention_mask.reshape(batch_size, 3 * seq_length)\n",
        "        device = stacked_inputs.device\n",
        "        # we feed in the input embeddings (not word indices as in NLP) to the model\n",
        "        encoder_outputs = self.encoder(\n",
        "            inputs_embeds=stacked_inputs,\n",
        "            attention_mask=stacked_attention_mask,\n",
        "            position_ids=torch.zeros(stacked_attention_mask.shape, device=device, dtype=torch.long),\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        x = encoder_outputs[0]\n",
        "\n",
        "        # reshape x so that the second dimension corresponds to the original\n",
        "        # returns (0), states (1), or actions (2); i.e. x[:,1,t] is the token for s_t\n",
        "        x = x.reshape(batch_size, seq_length, 3, self.hidden_size).permute(0, 2, 1, 3)\n",
        "\n",
        "        # TODO: get predictions, use predict_state, predict_action and predict_return heads\n",
        "        return_preds = <TODO>  # predict next return given state and action\n",
        "        state_preds = <TODO>  # predict next state given state and action\n",
        "        action_preds = <TODO>  # predict next action given state\n",
        "        if not return_dict:\n",
        "            return (state_preds, action_preds, return_preds)\n",
        "\n",
        "        return DecisionTransformerOutput(\n",
        "            last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            state_preds=state_preds,\n",
        "            action_preds=action_preds,\n",
        "            return_preds=return_preds,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmTRGPKYUVFG"
      },
      "source": [
        "### Step 6: Extending the Decision Transformer Model to include a loss function\n",
        "\n",
        "In order to train the model with the ü§ó trainer class, we first need to ensure the dictionary it returns contains a loss, in this case L-2 norm of the models action predictions and the targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwZp7hhFUh5u"
      },
      "outputs": [],
      "source": [
        "class TrainableDT(DecisionTransformerModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        output = super().forward(**kwargs)\n",
        "        # add the DT loss\n",
        "        action_preds = output[1]\n",
        "        action_targets = kwargs[\"actions\"]\n",
        "        attention_mask = kwargs[\"attention_mask\"]\n",
        "        act_dim = action_preds.shape[2]\n",
        "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
        "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
        "\n",
        "        loss = torch.mean((action_preds - action_targets) ** 2)\n",
        "\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def original_forward(self, **kwargs):\n",
        "        return super().forward(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIJCY3b3pQAh"
      },
      "outputs": [],
      "source": [
        "collator = DecisionTransformerGymDataCollator(dataset[\"train\"])\n",
        "\n",
        "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
        "model = TrainableDT(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJJ2mr_cU4eE"
      },
      "source": [
        "### Step 7: Defining the training hyperparameters and training the model\n",
        "Here, we define the training hyperparameters and our Trainer class that we'll use to train our Decision Transformer model.\n",
        "\n",
        "This step takes about an hour, so you may leave it running. Note the authors train for at least 3 hours, so the results presented here are not as performant as the models hosted on the ü§ó hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNzzKWuuU9I4"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"output/\",\n",
        "    remove_unused_columns=False,\n",
        "    num_train_epochs=120,\n",
        "    per_device_train_batch_size=64,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=1e-4,\n",
        "    warmup_ratio=0.1,\n",
        "    optim=\"adamw_torch\",\n",
        "    max_grad_norm=0.25,\n",
        "    report_to=\"none\"  # Disable all logging services\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNaj6bOkp3bt"
      },
      "source": [
        "### Step 8: Visualize the performance of the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0NhIn4up26c"
      },
      "outputs": [],
      "source": [
        "import mujoco_py\n",
        "import gym\n",
        "\n",
        "from colabgymrender.recorder import Recorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-izl68BqUG5"
      },
      "outputs": [],
      "source": [
        "# Function that gets an action from the model using autoregressive prediction with a window of the previous 20 timesteps.\n",
        "def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n",
        "    # This implementation does not condition on past rewards\n",
        "\n",
        "    states = states.reshape(1, -1, model.config.state_dim)\n",
        "    actions = actions.reshape(1, -1, model.config.act_dim)\n",
        "    returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
        "    timesteps = timesteps.reshape(1, -1)\n",
        "\n",
        "    states = states[:, -model.config.max_length :]\n",
        "    actions = actions[:, -model.config.max_length :]\n",
        "    returns_to_go = returns_to_go[:, -model.config.max_length :]\n",
        "    timesteps = timesteps[:, -model.config.max_length :]\n",
        "    padding = model.config.max_length - states.shape[1]\n",
        "    # pad all tokens to sequence length\n",
        "    attention_mask = torch.cat([torch.zeros(padding), torch.ones(states.shape[1])])\n",
        "    attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
        "    states = torch.cat([torch.zeros((1, padding, model.config.state_dim)), states], dim=1).float()\n",
        "    actions = torch.cat([torch.zeros((1, padding, model.config.act_dim)), actions], dim=1).float()\n",
        "    returns_to_go = torch.cat([torch.zeros((1, padding, 1)), returns_to_go], dim=1).float()\n",
        "    timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long), timesteps], dim=1)\n",
        "\n",
        "    state_preds, action_preds, return_preds = model.original_forward(\n",
        "        states=states,\n",
        "        actions=actions,\n",
        "        rewards=rewards,\n",
        "        returns_to_go=returns_to_go,\n",
        "        timesteps=timesteps,\n",
        "        attention_mask=attention_mask,\n",
        "        return_dict=False,\n",
        "    )\n",
        "\n",
        "    return action_preds[0, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc2PGGtMe3X2"
      },
      "outputs": [],
      "source": [
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFPuiNy-qWnP"
      },
      "outputs": [],
      "source": [
        "# build the environment\n",
        "directory = './video'\n",
        "model = model.to(\"cpu\")\n",
        "env = gym.make(\"HalfCheetah-v3\")\n",
        "env = Recorder(env, directory, fps=30)\n",
        "max_ep_len = 1000\n",
        "device = \"cpu\"\n",
        "scale = 1000.0  # normalization for rewards/returns\n",
        "TARGET_RETURN = 12000 / scale  # evaluation is conditioned on a return of 12000, scaled accordingly\n",
        "\n",
        "state_mean = collator.state_mean.astype(np.float32)\n",
        "state_std = collator.state_std.astype(np.float32)\n",
        "print(state_mean)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "# Create the decision transformer model\n",
        "\n",
        "state_mean = torch.from_numpy(state_mean).to(device=device)\n",
        "state_std = torch.from_numpy(state_std).to(device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysPH9rtRqY-g"
      },
      "outputs": [],
      "source": [
        "# Interact with the environment and create a video\n",
        "episode_return, episode_length = 0, 0\n",
        "state = env.reset()\n",
        "target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
        "states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
        "actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
        "rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
        "\n",
        "timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
        "for t in range(max_ep_len):\n",
        "    actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
        "    rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
        "\n",
        "    action = get_action(\n",
        "        model,\n",
        "        (states - state_mean) / state_std,\n",
        "        actions,\n",
        "        rewards,\n",
        "        target_return,\n",
        "        timesteps,\n",
        "    )\n",
        "    actions[-1] = action\n",
        "    action = action.detach().cpu().numpy()\n",
        "\n",
        "    state, reward, done, _ = env.step(action)\n",
        "\n",
        "    cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
        "    states = torch.cat([states, cur_state], dim=0)\n",
        "    rewards[-1] = reward\n",
        "\n",
        "    pred_return = target_return[0, -1] - (reward / scale)\n",
        "    target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
        "    timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
        "\n",
        "    episode_return += reward\n",
        "    episode_length += 1\n",
        "\n",
        "    if done:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn1Zy7x9qbnI"
      },
      "outputs": [],
      "source": [
        "# Play the video\n",
        "env.play()\n",
        "# If you want to convert the video:\n",
        "# !ffmpeg -i {your_video} -vcodec h264 replay.mp4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EgwxEjOxkBS"
      },
      "source": [
        "## Some additional challenges üèÜ\n",
        "Congratulations, you've just trained your first Decision Transformer ü•≥.\n",
        "\n",
        "Now, the best way to learn **is to try things by your own**! Why not trying with another environment?\n",
        "\n",
        "We provide datasets for some other environments:\n",
        "* [Walker2D](https://huggingface.co/edbeeching/decision-transformer-gym-walker2d-expert)\n",
        "* [Hopper](https://huggingface.co/edbeeching/decision-transformer-gym-hopper-expert)\n",
        "\n",
        "\n",
        "Have fun!\n",
        "\n",
        "### Keep Learning, Stay awesome ü§ó\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
