{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "40409bdb-54f9-4e0a-95f8-a020fd34d7d0",
      "metadata": {
        "id": "40409bdb-54f9-4e0a-95f8-a020fd34d7d0"
      },
      "source": [
        "# Soft Actor-Critic (SAC) Homework\n",
        "\n",
        "Soft Actor-Critic (SAC) is an off-policy, maximum entropy algorithm used in Deep Reinforcement Learning. It follows the principles of policy iteration and consists of two main steps:\n",
        "\n",
        "1. **Policy Evaluation**: Learning the soft Q-values under the current policy. This is done by performing a SARSA-like temporal difference update on the critic.\n",
        "2. **Policy Improvement**: Improving the policy based on the soft Q-values. This involves backpropagating through the critic to change the policy such that the critic's output is maximized.\n",
        "\n",
        "Despite being developed a few years ago, SAC remains an effective algorithm, especially when using a high Replay Ratio (RR), which is the number of gradient update steps per environment step. However, increasing RR can lead to training instability, negatively affecting the training process.\n",
        "\n",
        "## Homework Tasks\n",
        "\n",
        "In this homework, you will address the issue of value overestimation associated with temporal difference learning, which is particularly noticeable in high RR settings. Your tasks are:\n",
        "\n",
        "1. **Fill-in the implementation of the SAC Algorithm**\n",
        "   - Develop the code for the policy evaluation and policy improvement steps of the SAC algorithm.\n",
        "   - Implement a simple remedy to mitigate training instabilities.\n",
        "\n",
        "2. **Train SAC Agent on the Ant Environment**\n",
        "   - Use the Ant environment to train your SAC agent and observe its performance.\n",
        "\n",
        "3. **Investigate Training Instabilities in High RR**\n",
        "   - Analyze the training instabilities that arise when using a high Replay Ratio.\n",
        "\n",
        "## References\n",
        "\n",
        "1. Haarnoja, T., et al. (2018). \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\" [arXiv:1812.05905](https://arxiv.org/abs/1812.05905)\n",
        "2. [OpenReview: Replay Ratio in Off-Policy Algorithms](https://openreview.net/pdf?id=OpC-9aBBVJe)\n",
        "3. Fujimoto, S., et al. (2018). \"Addressing Function Approximation Error in Actor-Critic Methods.\" [arXiv:1802.09477](https://arxiv.org/abs/1802.09477)\n",
        "4. [Training Instabilities in High Replay Ratio](https://arxiv.org/pdf/2403.05996)\n",
        "\n",
        "\n",
        "Ensure your implementation is clear and well-documented, and analyze the results thoroughly. Make sure you use gpu runtime to speed up computations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Provided code"
      ],
      "metadata": {
        "id": "K6TwttTzi076"
      },
      "id": "K6TwttTzi076"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[mujoco]"
      ],
      "metadata": {
        "id": "Ma7KDNTGGPG4"
      },
      "id": "Ma7KDNTGGPG4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aaaa1da-937e-460d-a440-e0b3d2779de3",
      "metadata": {
        "id": "8aaaa1da-937e-460d-a440-e0b3d2779de3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "from typing import OrderedDict\n",
        "import gymnasium as gym\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1000530-8f9a-4254-91f9-fed0579e67f2",
      "metadata": {
        "id": "c1000530-8f9a-4254-91f9-fed0579e67f2"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d6119c-bad7-475d-a4cc-ebf8f68e4edb",
      "metadata": {
        "id": "52d6119c-bad7-475d-a4cc-ebf8f68e4edb"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, env, capacity: int):\n",
        "        state_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.shape[0]\n",
        "        self.states = np.empty((capacity, state_dim), dtype=np.float32)\n",
        "        self.actions = np.empty((capacity, action_dim), dtype=np.float32)\n",
        "        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n",
        "        self.masks = np.empty((capacity, 1), dtype=np.float32)\n",
        "        self.next_states = np.empty((capacity, state_dim), dtype=np.float32)\n",
        "        self.size = 0\n",
        "        self.insert_index = 0\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def add(self, state: np.ndarray, action: np.ndarray, reward: float, next_state: np.ndarray, mask: float):\n",
        "        self.states[self.insert_index] = state\n",
        "        self.actions[self.insert_index] = action\n",
        "        self.rewards[self.insert_index] = reward\n",
        "        self.masks[self.insert_index] = mask\n",
        "        self.next_states[self.insert_index] = next_state\n",
        "        self.insert_index = (self.insert_index + 1) % self.capacity\n",
        "        self.size = min(self.size + 1, self.capacity)\n",
        "\n",
        "    def to_torch(self, array):\n",
        "        return torch.from_numpy(array).float()\n",
        "\n",
        "    def sample(self, batch_size: int, num_batches: int):\n",
        "        indxs = np.random.randint(self.size, size=(num_batches, batch_size))\n",
        "        states = self.to_torch(self.states[indxs])\n",
        "        actions = self.to_torch(self.actions[indxs])\n",
        "        rewards = self.to_torch(self.rewards[indxs])\n",
        "        next_states = self.to_torch(self.next_states[indxs])\n",
        "        masks = self.to_torch(self.masks[indxs])\n",
        "        return states, actions, rewards, next_states, masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06252e83-0f0f-4636-b014-fff5e1b775db",
      "metadata": {
        "id": "06252e83-0f0f-4636-b014-fff5e1b775db"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def get_masks(terminates, truncates):\n",
        "    masks = 0.0\n",
        "    if not terminates or truncates:\n",
        "        masks = 1.0\n",
        "    return masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d41717b-cd32-4cb3-b803-7cda24736037",
      "metadata": {
        "id": "7d41717b-cd32-4cb3-b803-7cda24736037"
      },
      "outputs": [],
      "source": [
        "def weight_init(model):\n",
        "    if isinstance(model, nn.Linear):\n",
        "        nn.init.orthogonal_(model.weight.data)\n",
        "        model.bias.data.fill_(0.0)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):\n",
        "        super(Actor, self).__init__()\n",
        "        self.log_std_min = -10.0\n",
        "        self.log_std_max = 2.0\n",
        "        self.activation = nn.ReLU()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            self.activation,\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            self.activation,\n",
        "            nn.Linear(hidden_dim, 2 * action_dim))\n",
        "        self.apply(weight_init)\n",
        "\n",
        "    def forward(self, state):\n",
        "        mu, log_std = self.layers(state).chunk(2, dim=-1)\n",
        "        # cap log_std between log_std_min and log_std_max - good for stability\n",
        "        log_std = self.log_std_min + 0.5 * (self.log_std_max - self.log_std_min) * (torch.tanh(log_std) + 1)\n",
        "        return mu, log_std.exp()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 - Fill-in the implementation of the SAC Algorithm"
      ],
      "metadata": {
        "id": "feMPXC0xsu6s"
      },
      "id": "feMPXC0xsu6s"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1a - Implement a simple remedy to mitigate training instabilities\n",
        "\n",
        "Implement two versions of the critic network - with layer normalization and without.\n",
        "\n",
        "Common elements of the architecture:\n",
        "* Input: state and action\n",
        "* Output: value of the action-state pair - float\n",
        "* Depth: 2 hidden layers of size `hidden_dim`\n",
        "* Activation: `self.activation`\n",
        "\n",
        "Layer normalization should be applied after each hidden layer.\n"
      ],
      "metadata": {
        "id": "rQoKL8MDtLNf"
      },
      "id": "rQoKL8MDtLNf"
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256, use_layernorm: bool = False):\n",
        "        super(Critic, self).__init__()\n",
        "        self.activation = nn.ReLU()\n",
        "        if use_layernorm:\n",
        "            ####### TODO #######\n",
        "            # self.layers = ...\n",
        "            ####################\n",
        "        else:\n",
        "            ####### TODO #######\n",
        "            # self.layers = ...\n",
        "            ####################\n",
        "\n",
        "        self.apply(weight_init)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        state_action = torch.concat((state, action), dim=1)\n",
        "        q_value = self.layers(state_action)\n",
        "        return q_value\n",
        "\n",
        "class DoubleCritic(nn.Module):\n",
        "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256, use_layernorm: bool = False):\n",
        "        super(DoubleCritic, self).__init__()\n",
        "        self.critic1 = Critic(state_dim=state_dim, action_dim=action_dim, hidden_dim=hidden_dim, use_layernorm=use_layernorm)\n",
        "        self.critic2 = Critic(state_dim=state_dim, action_dim=action_dim, hidden_dim=hidden_dim, use_layernorm=use_layernorm)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        q1 = self.critic1(state, action)\n",
        "        q2 = self.critic2(state, action)\n",
        "        return q1, q2"
      ],
      "metadata": {
        "id": "LGoSj2ims3J0"
      },
      "id": "LGoSj2ims3J0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1b - Implement Update Steps of the SAC Algorithm\n",
        "Please fill missing parts of code in the following key sections of the SAC algorithm:\n",
        "1. `target_Q` computation in method `update_critic` - compute q-backup according to equations 3, 5, and 6 from [1]. Remeber you are using double q-learning setup and max entropy setting.\n",
        "2. `actor_loss` computation in method `update_actor` - compute actor loss according to equation 7 from [1]. Remeber you are using double q-learning setup.\n",
        "3. `alpha_loss` computation in method `update_actor` - compute alpha loss according to equation 18 from [1].\n",
        "\n",
        "**References**\n",
        " 1. Haarnoja, T., et al. (2018). \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\" [arXiv:1812.05905](https://arxiv.org/abs/1812.05905)"
      ],
      "metadata": {
        "id": "oOqudKaXjNrf"
      },
      "id": "oOqudKaXjNrf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a6e33b1-b8b8-4c6f-b603-893a2ceb9532",
      "metadata": {
        "id": "1a6e33b1-b8b8-4c6f-b603-893a2ceb9532"
      },
      "outputs": [],
      "source": [
        "def gaussian_logprob(noise, log_std):\n",
        "    residual = (-0.5 * noise.pow(2) - log_std).sum(-1, keepdim=True)\n",
        "    return residual - 0.5 * np.log(2 * np.pi) * noise.size(-1)\n",
        "\n",
        "def squash(pi, log_prob):\n",
        "    if pi is not None:\n",
        "        pi = torch.tanh(pi)\n",
        "    if log_prob is not None:\n",
        "        log_prob -= torch.log(nn.functional.relu(1 - pi.pow(2)) + 1e-6).sum(-1, keepdim=True)\n",
        "    return pi, log_prob\n",
        "\n",
        "class SAC(nn.Module):\n",
        "    def __init__(self, env, use_layernorm: bool = False):\n",
        "        super(SAC, self).__init__()\n",
        "        self.device = DEVICE\n",
        "        self.state_dim = env.observation_space.shape[0]\n",
        "        self.action_dim = env.action_space.shape[0]\n",
        "        self.hidden_dim = 256\n",
        "        self.learning_rate = 3e-3\n",
        "        self.discount = 0.99\n",
        "        self.target_entropy = -np.prod(self.action_dim) / 2\n",
        "        self.tau = 0.005\n",
        "        self.use_layernorm = use_layernorm\n",
        "        self.reset()\n",
        "        self.logger = {'q_value': [], 'temperature': [], 'entropy': [], 'critic_loss': [], 'actor_loss': [], 'returns': []}\n",
        "\n",
        "    def reset(self):\n",
        "        self.actor = Actor(self.state_dim, self.action_dim, self.hidden_dim).to(self.device)\n",
        "        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=self.learning_rate)\n",
        "        self.critic = DoubleCritic(self.state_dim, self.action_dim, self.hidden_dim, self.use_layernorm).to(self.device)\n",
        "        self.target_critic = copy.deepcopy(self.critic).to(self.device)\n",
        "        self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr=self.learning_rate)\n",
        "        self.log_alpha = torch.tensor(np.log(1.0)).to(self.device)\n",
        "        self.log_alpha.requires_grad = True\n",
        "        self.optimizer_log_alpha = torch.optim.Adam([self.log_alpha], lr=self.learning_rate, )\n",
        "\n",
        "    @property\n",
        "    def alpha(self):\n",
        "        return self.log_alpha.exp()\n",
        "\n",
        "    def get_action(self, state, return_logprob=False, temperature=1.0):\n",
        "        mu, std = self.actor(state)\n",
        "        noise = torch.randn_like(mu)\n",
        "        std = std * temperature\n",
        "        action = mu + noise * std\n",
        "        if return_logprob:\n",
        "            log_prob = gaussian_logprob(noise, std.log())\n",
        "            action, log_prob = squash(action, log_prob)\n",
        "            return action, log_prob\n",
        "        else:\n",
        "            action, _ = squash(action, None)\n",
        "            return action\n",
        "\n",
        "    def update(self, step, states, actions, rewards, next_states, masks):\n",
        "        for i in range(states.shape[0]):\n",
        "            self.update_critic(step, states[i], actions[i], rewards[i], next_states[i], masks[i])\n",
        "            self.update_actor(step, states[i])\n",
        "            self.update_target_critic(self.tau)\n",
        "\n",
        "    def update_target_critic(self, tau):\n",
        "        for param, target_param in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "    def update_critic(self, step, states, actions, rewards, next_states, masks):\n",
        "        with torch.no_grad():\n",
        "            ####### TODO #######\n",
        "\n",
        "            # target_Q = ...\n",
        "            ####################\n",
        "        current_Q1, current_Q2 = self.critic(states, actions)\n",
        "        critic_loss = nn.functional.mse_loss(current_Q1, target_Q) + nn.functional.mse_loss(current_Q2, target_Q)\n",
        "        self.optimizer_critic.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.optimizer_critic.step()\n",
        "        if step == 1:\n",
        "            self.logger['q_value'].append(current_Q1.mean().detach().item())\n",
        "            self.logger['critic_loss'].append(critic_loss.mean().detach().item())\n",
        "\n",
        "    def update_actor(self, step, states):\n",
        "        actions, log_probs = self.get_action(states, return_logprob=True, temperature=1.0)\n",
        "        ####### TODO #######\n",
        "\n",
        "        # actor_loss = ...\n",
        "        ####################\n",
        "        self.optimizer_actor.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.optimizer_actor.step()\n",
        "        self.optimizer_log_alpha.zero_grad()\n",
        "        ####### TODO #######\n",
        "\n",
        "        # alpha_loss = ...\n",
        "        ####################\n",
        "        alpha_loss.backward()\n",
        "        self.optimizer_log_alpha.step()\n",
        "        if step == 1:\n",
        "            self.logger['entropy'].append(entropy.detach().item())\n",
        "            self.logger['temperature'].append(self.alpha.detach().item())\n",
        "            self.logger['actor_loss'].append(actor_loss.detach().item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4479c7b6-33ab-44d3-995f-2d0f69da8e28",
      "metadata": {
        "id": "4479c7b6-33ab-44d3-995f-2d0f69da8e28"
      },
      "source": [
        "# Task 2 - Train SAC Agent on the Ant Environment\n",
        "Train the following models:\n",
        "\n",
        "1. use_layernorm = False, replay_ratio = 1\n",
        "2. use_layernorm = False, replay_ratio = 8\n",
        "3. use_layernorm = True, replay_ratio = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "158f5d80-7446-4e73-8438-f57a7f85cea4",
      "metadata": {
        "id": "158f5d80-7446-4e73-8438-f57a7f85cea4"
      },
      "outputs": [],
      "source": [
        "def train(init_steps: int = 1000,\n",
        "          training_steps: int = 50000,\n",
        "          replay_ratio: int = 8,\n",
        "          use_layernorm: bool = False):\n",
        "\n",
        "    set_seed(0)\n",
        "    env = gym.make('Ant-v4')\n",
        "\n",
        "    agent = SAC(env, use_layernorm=use_layernorm).to(DEVICE)\n",
        "    agent = torch.compile(agent)\n",
        "    buffer = ReplayBuffer(env, training_steps)\n",
        "\n",
        "    state, _ = env.reset(seed=np.random.randint(0,1e7))\n",
        "    state = torch.from_numpy(state).float().to(DEVICE)\n",
        "\n",
        "    returns = 0.0\n",
        "    episode_step = 0\n",
        "\n",
        "    for step in range(1, training_steps + 1):\n",
        "        if step >= init_steps:\n",
        "            action = agent.get_action(state).detach().cpu().numpy()\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        next_state, reward, terminal, truncate, _ = env.step(action)\n",
        "        next_state = torch.from_numpy(next_state).float().to(DEVICE)\n",
        "\n",
        "        mask = get_masks(terminal, truncate)\n",
        "\n",
        "        returns += reward\n",
        "        episode_step += 1\n",
        "\n",
        "        buffer.add(state.cpu().numpy(), action, reward, next_state.cpu().numpy(), mask)\n",
        "\n",
        "        if step >= init_steps:\n",
        "            states, actions, rewards, next_states, masks = buffer.sample(256, replay_ratio)\n",
        "            states = states.to(DEVICE)\n",
        "            actions = actions.to(DEVICE)\n",
        "            rewards = rewards.to(DEVICE)\n",
        "            next_states = next_states.to(DEVICE)\n",
        "            masks = masks.to(DEVICE)\n",
        "\n",
        "            agent.update(episode_step, states, actions, rewards, next_states, masks)\n",
        "\n",
        "        if terminal or truncate:\n",
        "            state, _ = env.reset(seed=np.random.randint(0, 1e7))\n",
        "            state = torch.from_numpy(state).float().to(DEVICE)\n",
        "\n",
        "            if len(agent.logger['q_value']) > 0:\n",
        "                agent.logger['returns'].append(returns)\n",
        "                print(f\"TrainStep: {step} Returns: {np.round(returns, 2)} Q-values: {np.round(agent.logger['q_value'][-1], 2)} Temperature: {np.round(agent.logger['temperature'][-1], 2)} Entropy: {np.round(agent.logger['entropy'][-1], 2)}\")\n",
        "\n",
        "            returns = 0.0\n",
        "            episode_step = 0\n",
        "        else:\n",
        "            state = next_state\n",
        "\n",
        "    return agent.logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b11aefb8-f555-4081-a824-34ce77017652",
      "metadata": {
        "id": "b11aefb8-f555-4081-a824-34ce77017652"
      },
      "outputs": [],
      "source": [
        "# train low rr agent ~ 10 min\n",
        "history_lowrr = train(replay_ratio=1, use_layernorm=False)\n",
        "np.savez(\"history_lowrr.npz\", history_lowrr)\n",
        "files.download(\"history_lowrr.npz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eaa8ce3-ad44-44b4-9809-e4dbdcf77d3f",
      "metadata": {
        "id": "3eaa8ce3-ad44-44b4-9809-e4dbdcf77d3f"
      },
      "outputs": [],
      "source": [
        "# train high rr agent ~ 1h\n",
        "history_noln = train(use_layernorm=False)\n",
        "np.savez(\"history_noln.npz\", history_noln)\n",
        "files.download(\"history_noln.npz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6bc4d54-79e0-4d8f-a7cf-8d3518e20219",
      "metadata": {
        "id": "f6bc4d54-79e0-4d8f-a7cf-8d3518e20219"
      },
      "outputs": [],
      "source": [
        "# train high rr LN agent ~ 1h\n",
        "history_ln = train(use_layernorm=True)\n",
        "np.savez(\"history_ln.npz\", history_ln)\n",
        "files.download(\"history_ln.npz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecf010ba-7336-4b9b-b39a-a70c6732cab0",
      "metadata": {
        "id": "ecf010ba-7336-4b9b-b39a-a70c6732cab0"
      },
      "outputs": [],
      "source": [
        "# print average performance\n",
        "\n",
        "print('Low RR, No LayerNorm: ', np.asarray(history_lowrr['returns']).mean())\n",
        "print('High RR, No LayerNorm: ', np.asarray(history_noln['returns']).mean())\n",
        "print('High RR, LayerNorm: ', np.asarray(history_ln['returns']).mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 - Investigate Training Instabilities in High RR"
      ],
      "metadata": {
        "id": "LJJksngXnIo8"
      },
      "id": "LJJksngXnIo8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3a (coding)\n",
        "Graph the following timeseries for all tested models:\n",
        "\n",
        "1. Returns (Y-axis), Episode (X-axis)\n",
        "2. Q-values (Y-axis), Episode (X-axis)\n",
        "3. Temperature (Y-axis), Episode (X-axis)\n",
        "4. Entropy (Y-axis), Episode (X-axis)\n",
        "\n",
        "The task will be graded by **clarity of the presentation** and its **esthetic values**. The graph should provide all the information needed to observe properites and answer the quesions described below."
      ],
      "metadata": {
        "id": "au-qA_0zeriO"
      },
      "id": "au-qA_0zeriO"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cQJI5wrbeudM"
      },
      "id": "cQJI5wrbeudM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "22025e7c-1598-407e-9160-17c51d2bb594",
      "metadata": {
        "id": "22025e7c-1598-407e-9160-17c51d2bb594"
      },
      "source": [
        "\n",
        "\n",
        "## Task 3b (writing)\n",
        "Investigate the data and describe the issues that you observe. In particular, focus on the contrast between tested model variations in the following contexts:\n",
        "\n",
        "1. Does the critic learn correct Q-values? Does it overestimate or underestimate?\n",
        "2. Is the temperature mechanism enough to stabilize the entropy?\n",
        "3. What happens to the entropy temperature parameter? Why would it take \"large\" values (ie. > 10)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3c (writing)\n",
        "In previous questions we compare sum of episodic rewards to critic output at starting state. Is this approach correct given that our critic learns soft Q-values? What are the problems with this approach?\n",
        "\n"
      ],
      "metadata": {
        "id": "K7_izqqGnqpD"
      },
      "id": "K7_izqqGnqpD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3d (writing - bonus question)\n",
        "Why would using layer normalization help with value overestimation in high RR?"
      ],
      "metadata": {
        "id": "sqKyuCf4nski"
      },
      "id": "sqKyuCf4nski"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "947642a2-8369-4ab0-8925-15ff42170171",
      "metadata": {
        "id": "947642a2-8369-4ab0-8925-15ff42170171"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}