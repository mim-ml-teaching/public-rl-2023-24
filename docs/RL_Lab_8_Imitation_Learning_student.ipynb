{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-OmTFM5jfnL"
      },
      "source": [
        "<center><img src='https://i.postimg.cc/TPR1n1rp/AI-Tech-PL-RGB.png' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Programu Operacyjnego Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://i.postimg.cc/Gpq2KRQz/logotypy-aitech.jpg'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x1m0s-3jdQp"
      },
      "source": [
        "# Lab 08: Imitation Learning\n",
        "\n",
        "In this lab, we look into the problem of learning from expert demonstrations.\n",
        "\n",
        "- Find a policy $\\pi(a | s)$ that best imitates the expert policy $\\pi^*(a | s)$ in the given environment.\n",
        "- It's worth noting, that we don't need access to the environment rewards.\n",
        "\n",
        "Major Imitation Learning techniques are:\n",
        "\n",
        "1. Behavioural Cloning,\n",
        "1. Imitation Learning via Interactive Demonstrator e.g. SMILe (Ross and Bagnell, 2010) or DAgger (Ross et al., 2011),\n",
        "1. Inverse Reinforcement Learning -- out of scope of this lab.\n",
        "\n",
        "We will solve the Ant problem, shown below, examining the first two approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjqgLL8EjjAh"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yt4UPVjXjZKI"
      },
      "outputs": [],
      "source": [
        "!pip -q install gymnasium[mujoco]\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEJxM_OWyIiu",
        "outputId": "9cb00861-b531-4884-fc22-bdc039a930c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'sample-factory' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/alex-petrenko/sample-factory.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BZL76yoD36R2"
      },
      "outputs": [],
      "source": [
        "!pip install -q sample-factory[mujoco]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjfRlCiT4SOV",
        "outputId": "b1c3f85b-545c-46aa-8d50-bbbf27b6f315"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/bartek/expert_checkpoint/sample-factory\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bartek/anaconda3/envs/rl/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd sample-factory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iGPfs9zjpkQ"
      },
      "source": [
        "## Download Expert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Urgn_w9OyKJ-",
        "outputId": "5b0f7622-c8cc-4757-ee5b-a334004f8772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/bartek/anaconda3/envs/rl/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
            "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "Cloning https://huggingface.co/LLParallax/sf_Ant into local empty directory.\n",
            "\u001b[37m\u001b[1m[2024-04-25 17:19:15,774][150423] The repository LLParallax/sf_Ant has been cloned to ./train_dir/sf_Ant\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m sample_factory.huggingface.load_from_hub -r LLParallax/sf_Ant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jAKmX9wQmJKM"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "import torch\n",
        "\n",
        "from sample_factory.algo.learning.learner import Learner\n",
        "from sample_factory.algo.utils.env_info import extract_env_info\n",
        "from sample_factory.algo.utils.make_env import make_env_func_batched\n",
        "from sample_factory.algo.utils.rl_utils import prepare_and_normalize_obs\n",
        "from sample_factory.cfg.arguments import load_from_checkpoint\n",
        "from sample_factory.model.actor_critic import create_actor_critic\n",
        "from sample_factory.model.model_utils import get_rnn_size\n",
        "from sample_factory.utils.attr_dict import AttrDict\n",
        "from sample_factory.utils.typing import Config\n",
        "\n",
        "\n",
        "def create_expert(cfg):\n",
        "    cfg = load_from_checkpoint(cfg)\n",
        "\n",
        "    cfg.num_envs = 1\n",
        "\n",
        "    env = make_env_func_batched(\n",
        "        cfg, env_config=AttrDict(worker_index=0, vector_index=0, env_id=0), render_mode=None\n",
        "    )\n",
        "\n",
        "    if hasattr(env.unwrapped, \"reset_on_init\"):\n",
        "        # reset call ruins the demo recording for VizDoom\n",
        "        env.unwrapped.reset_on_init = False\n",
        "\n",
        "    actor_critic = create_actor_critic(cfg, env.observation_space, env.action_space)\n",
        "    actor_critic.eval()\n",
        "\n",
        "    device = torch.device(\"cpu\" if cfg.device == \"cpu\" else \"cuda\")\n",
        "    actor_critic.model_to_device(device)\n",
        "\n",
        "    policy_id = cfg.policy_index\n",
        "    name_prefix = dict(latest=\"checkpoint\", best=\"best\")[cfg.load_checkpoint_kind]\n",
        "    checkpoints = Learner.get_checkpoints(Learner.checkpoint_dir(cfg, policy_id), f\"{name_prefix}_*\")\n",
        "    checkpoint_dict = Learner.load_checkpoint(checkpoints, device)\n",
        "    actor_critic.load_state_dict(checkpoint_dict[\"model\"])\n",
        "    return actor_critic\n",
        "\n",
        "\n",
        "def get_expert_actions(obs, cfg: Config, actor_critic, env, env_info, device):\n",
        "    rnn_states = torch.zeros([env.num_agents, get_rnn_size(cfg)], dtype=torch.float32, device=device)\n",
        "\n",
        "    obs = {\"obs\": obs}\n",
        "    with torch.no_grad():\n",
        "        normalized_obs = prepare_and_normalize_obs(actor_critic, obs)\n",
        "        policy_outputs = actor_critic(normalized_obs, rnn_states)\n",
        "\n",
        "        # sample actions from the distribution by default\n",
        "        actions = policy_outputs[\"actions\"]\n",
        "    return actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2naspguhjm9h"
      },
      "source": [
        "## Load expert model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoaZGg2kiYWl",
        "outputId": "40162a21-cf1e-45ae-df19-e9e200757878"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33m[2024-04-25 17:19:35,658][147072] Environment mujoco_hopper already registered, overwriting...\u001b[0m\n",
            "\u001b[33m[2024-04-25 17:19:35,660][147072] Environment mujoco_halfcheetah already registered, overwriting...\u001b[0m\n",
            "\u001b[33m[2024-04-25 17:19:35,660][147072] Environment mujoco_humanoid already registered, overwriting...\u001b[0m\n",
            "\u001b[33m[2024-04-25 17:19:35,661][147072] Environment mujoco_ant already registered, overwriting...\u001b[0m\n",
            "\u001b[33m[2024-04-25 17:19:35,661][147072] Environment mujoco_standup already registered, overwriting...\u001b[0m\n",
            "\u001b[33m[2024-04-25 17:19:35,662][147072] Environment mujoco_doublependulum already registered, overwriting...\u001b[0m\n",
            "\u001b[33m[2024-04-25 17:19:35,662][147072] Environment mujoco_pendulum already registered, overwriting...\u001b[0m\n",
            "\u001b[33m[2024-04-25 17:19:35,662][147072] Environment mujoco_reacher already registered, overwriting...\u001b[0m\n",
            "\u001b[33m[2024-04-25 17:19:35,663][147072] Environment mujoco_walker already registered, overwriting...\u001b[0m\n",
            "\u001b[33m[2024-04-25 17:19:35,663][147072] Environment mujoco_pusher already registered, overwriting...\u001b[0m\n",
            "\u001b[33m[2024-04-25 17:19:35,663][147072] Environment mujoco_swimmer already registered, overwriting...\u001b[0m\n",
            "\u001b[33m[2024-04-25 17:19:35,703][147072] Loading existing experiment configuration from train_dir/sf_Ant/config.json\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,704][147072] Overriding arg 'experiment' with value 'sf_Ant' passed from command line\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,704][147072] Overriding arg 'train_dir' with value 'train_dir' passed from command line\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,705][147072] Adding new argument 'fps'=0 that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,705][147072] Adding new argument 'eval_env_frameskip'=None that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,706][147072] Adding new argument 'no_render'=True that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,706][147072] Adding new argument 'save_video'=False that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,706][147072] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,706][147072] Adding new argument 'video_name'=None that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,707][147072] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,707][147072] Adding new argument 'max_num_episodes'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,707][147072] Adding new argument 'push_to_hub'=False that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,707][147072] Adding new argument 'hf_repository'=None that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,708][147072] Adding new argument 'policy_index'=0 that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,708][147072] Adding new argument 'eval_deterministic'=False that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,708][147072] Adding new argument 'train_script'=None that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,708][147072] Adding new argument 'enjoy_script'=None that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,709][147072] Adding new argument 'sample_env_episodes'=256 that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,709][147072] Adding new argument 'csv_folder_name'=None that is not in the saved config file!\u001b[0m\n",
            "/home/bartek/anaconda3/envs/rl/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "/home/bartek/anaconda3/envs/rl/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "\u001b[36m[2024-04-25 17:19:35,829][147072] RunningMeanStd input shape: (27,)\u001b[0m\n",
            "\u001b[36m[2024-04-25 17:19:35,839][147072] RunningMeanStd input shape: (1,)\u001b[0m\n",
            "\u001b[33m[2024-04-25 17:19:36,054][147072] Loading state from checkpoint train_dir/sf_Ant/checkpoint_p0/checkpoint_000019544_10006528.pth...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from sample_factory.cfg.arguments import parse_full_cfg, parse_sf_args\n",
        "from sample_factory.envs.env_utils import register_env\n",
        "from sf_examples.mujoco.mujoco_params import add_mujoco_env_args, mujoco_override_defaults\n",
        "from sf_examples.mujoco.train_mujoco import register_mujoco_components\n",
        "from sf_examples.mujoco.mujoco_utils import MUJOCO_ENVS, make_mujoco_env\n",
        "\n",
        "\n",
        "def register_mujoco_components():\n",
        "    for env in MUJOCO_ENVS:\n",
        "        register_env(env.name, make_mujoco_env)\n",
        "\n",
        "\n",
        "register_mujoco_components()\n",
        "argv = [\"--algo=APPO\", \"--env=mujoco_ant\", \"--experiment=sf_Ant\", \"--train_dir=train_dir\", \"--no_render\"]\n",
        "parser, partial_cfg = parse_sf_args(argv=argv, evaluation=True)\n",
        "add_mujoco_env_args(partial_cfg.env, parser)\n",
        "mujoco_override_defaults(partial_cfg.env, parser)\n",
        "cfg = parse_full_cfg(parser, argv=argv)\n",
        "expert = create_expert(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrGdCIr0iaLt"
      },
      "source": [
        "## Helpers\n",
        "collecting data  \n",
        "\n",
        "evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPi_r6R8ibVF",
        "outputId": "7cdbe945-4e20-4fd7-a945-53467e06b78f"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "from IPython import display as ipydisplay\n",
        "\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import animation\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_policy (env, model, total_steps=10000, verbose=True):\n",
        "    obs_array = np.empty([total_steps, *env.observation_space.shape])\n",
        "    act_array = np.empty([total_steps, env.action_space.shape[0]])\n",
        "    rew_array = np.empty([total_steps, 1])\n",
        "    done_array = np.empty([total_steps, 1])\n",
        "\n",
        "    iter_time = time.time()\n",
        "    done = True\n",
        "    for i in range(total_steps):\n",
        "        if verbose and (i + 1) % 1000 == 0:\n",
        "            steps_per_second = 1000 / (time.time() - iter_time)\n",
        "            print(f'Step {i + 1}/{total_steps}, Steps per second: {steps_per_second}')\n",
        "            iter_time = time.time()\n",
        "\n",
        "        if done:\n",
        "            obs, info = env.reset()\n",
        "\n",
        "        act = model(torch.from_numpy(obs).unsqueeze(0).float())[0].detach().cpu().numpy()\n",
        "        obs_, rew, terminated, truncated, _ = env.step(act)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        obs_array[i] = obs\n",
        "        act_array[i] = act\n",
        "        rew_array[i] = rew\n",
        "        done_array[i] = float(done)\n",
        "\n",
        "        obs = obs_\n",
        "\n",
        "    return obs_array, act_array, rew_array, done_array\n",
        "\n",
        "def calculate_returns(rew, done):\n",
        "    rew_cumsum = np.cumsum(rew)[:, None]\n",
        "    ret_cumsum = rew_cumsum * done\n",
        "    ret_cumsum_trimed = ret_cumsum[np.nonzero(ret_cumsum)]\n",
        "    ret_cumsum_trimed[1:] -= ret_cumsum_trimed[:-1]\n",
        "    return ret_cumsum_trimed\n",
        "\n",
        "def evaluate_agent(env, model, verbose=False):\n",
        "    _, _, rew, done = run_policy(env, model, total_steps=50000, verbose=verbose)\n",
        "    rets = calculate_returns(rew, done)\n",
        "\n",
        "    print(f'Num. episodes: {len(rets)}')\n",
        "    print(f'Avg. return: {np.mean(rets)}')\n",
        "    print(f'Max. return: {np.max(rets)}')\n",
        "    print(f'Min. return: {np.min(rets)}')\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_frames(eval_env, model, num_frames=2000):\n",
        "    state, _ = eval_env.reset()\n",
        "    state = torch.from_numpy(np.array(state)).float()\n",
        "    frames = []\n",
        "\n",
        "    for _ in range(num_frames):\n",
        "        frames.append(eval_env.render())\n",
        "\n",
        "        action = model(state.unsqueeze(0))[0]\n",
        "        next_state, reward, terminal, truncate, info = eval_env.step(action.detach().cpu().numpy())\n",
        "\n",
        "        if terminal or truncate:\n",
        "            state, _ = eval_env.reset()\n",
        "        state = next_state\n",
        "        state = torch.from_numpy(np.array(state)).float()\n",
        "\n",
        "    return frames\n",
        "\n",
        "def display_frames_as_video(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a video.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "    ipydisplay.display(ipydisplay.HTML(anim.to_jshtml()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSiZuQnzijMY"
      },
      "source": [
        "## 1. Behavior Clonning\n",
        "\n",
        "Algorithm\n",
        "\n",
        "1. Collect the expert data.\n",
        "2. Fit the model (classifier/regressor) to the expert data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGkOlSfWj5Cs"
      },
      "source": [
        "### Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "v8e3RXYsij0O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_shape, output_size, hidden_sizes=(256, 256), hidden_activation=nn.Tanh(), output_activation=None, l2_weight=0.0001):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential()\n",
        "\n",
        "        # Input layer\n",
        "        self.layers.add_module(\"input\", nn.Linear(input_shape, hidden_sizes[0]))\n",
        "        self.layers.add_module(\"input_activation\", hidden_activation)\n",
        "\n",
        "        # Hidden layers\n",
        "        layer_sizes = zip(hidden_sizes[:-1], hidden_sizes[1:])\n",
        "        for i, (h1, h2) in enumerate(layer_sizes):\n",
        "            self.layers.add_module(f\"hidden_{i}\", nn.Linear(h1, h2))\n",
        "            self.layers.add_module(f\"activation_{i}\", hidden_activation)\n",
        "\n",
        "        # Output layer\n",
        "        self.layers.add_module(\"output\", nn.Linear(hidden_sizes[-1], output_size))\n",
        "        if output_activation is not None:\n",
        "            self.layers.add_module(\"output_activation\", output_activation)\n",
        "\n",
        "        # Regularization\n",
        "        self.l2_weight = l2_weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "    def l2_regularization(self):\n",
        "        l2_reg = None\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                if l2_reg is None:\n",
        "                    l2_reg = param.norm(2)\n",
        "                else:\n",
        "                    l2_reg = l2_reg + param.norm(2)\n",
        "        return self.l2_weight * l2_reg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmDBOLRIilgz"
      },
      "source": [
        "### Function for training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ft4CtoNgik6E"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "def train(obs, act, model, num_epochs=10, batch_size=32):\n",
        "    obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
        "    act_tensor = torch.tensor(act, dtype=torch.float32)\n",
        "\n",
        "    dataset = TensorDataset(obs_tensor, act_tensor)\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    loss_fn = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, (x_batch, y_batch) in enumerate(data_loader):\n",
        "            # Forward pass\n",
        "            y_pred = model(x_batch)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(y_pred, y_batch) + model.l2_regularization()\n",
        "\n",
        "            # Zero gradients, perform a backward pass, and update the weights.\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Print loss every epoch\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cQA_gW6iofY",
        "outputId": "cee44cef-4753-4eb9-c4bf-c618147ae05a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1000/10000, Steps per second: 735.2488033790245\n",
            "Step 2000/10000, Steps per second: 1089.0234187975602\n",
            "Step 3000/10000, Steps per second: 1090.9194947281248\n",
            "Step 4000/10000, Steps per second: 1073.1327653527585\n",
            "Step 5000/10000, Steps per second: 1118.6519966917613\n",
            "Step 6000/10000, Steps per second: 1128.1218455424262\n",
            "Step 7000/10000, Steps per second: 1124.9511859140787\n",
            "Step 8000/10000, Steps per second: 1112.3872609329744\n",
            "Step 9000/10000, Steps per second: 1122.014965000432\n",
            "Step 10000/10000, Steps per second: 1125.2357820505977\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('Ant-v4')\n",
        "env.num_agents = 1\n",
        "env_info = extract_env_info(env, cfg)\n",
        "device = torch.device(\"cpu\" if cfg.device == \"cpu\" else \"cuda\")\n",
        "collected_data = run_policy(env, functools.partial(get_expert_actions, cfg=cfg, actor_critic=expert, env=env, env_info=env_info, device=device), total_steps=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTsFAoG4iqN4",
        "outputId": "8aa33ce3-4524-4879-e8ca-0ac78131db95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bartek/anaconda3/envs/rl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.017476055771112442\n",
            "Epoch 2/10, Loss: 0.014620059169828892\n",
            "Epoch 3/10, Loss: 0.009299281984567642\n",
            "Epoch 4/10, Loss: 0.01285579800605774\n",
            "Epoch 5/10, Loss: 0.013694003224372864\n",
            "Epoch 6/10, Loss: 0.012380083091557026\n",
            "Epoch 7/10, Loss: 0.014809663407504559\n",
            "Epoch 8/10, Loss: 0.012385744601488113\n",
            "Epoch 9/10, Loss: 0.01006702333688736\n",
            "Epoch 10/10, Loss: 0.01109317410737276\n"
          ]
        }
      ],
      "source": [
        "obs, act, rewards, dones = collected_data\n",
        "\n",
        "# EXERCISE: Create model\n",
        "model = ...\n",
        "\n",
        "train(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCjtebL4irnz",
        "outputId": "c21f20d6-0e62-4b50-9056-70b4ca1bbfa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num. episodes: 82\n",
            "Avg. return: 2547.0173836750105\n",
            "Max. return: 5538.0024825983855\n",
            "Min. return: 14.634580395818375\n"
          ]
        }
      ],
      "source": [
        "evaluate_agent(env, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgaDRcOgisq5"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Discuss the questions\n",
        "\n",
        "1. In principle, do we need the expert policy for BC?\n",
        "\n",
        "2. What are the problems with BC?\n",
        "\n",
        "3. How can we help BC do better?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0Z_UzUBithV",
        "outputId": "11673a1f-422f-40a0-d86d-3a14eaaa535d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1000/10000, Steps per second: 803.912627892673\n",
            "Step 2000/10000, Steps per second: 769.5650107784502\n",
            "Step 3000/10000, Steps per second: 769.4930061751282\n",
            "Step 4000/10000, Steps per second: 788.1560420823024\n",
            "Step 5000/10000, Steps per second: 773.1042623453627\n",
            "Step 6000/10000, Steps per second: 775.3428526851833\n",
            "Step 7000/10000, Steps per second: 823.1821436197606\n",
            "Step 8000/10000, Steps per second: 803.5847166735766\n",
            "Step 9000/10000, Steps per second: 810.7443481548592\n",
            "Step 10000/10000, Steps per second: 762.390307547844\n"
          ]
        }
      ],
      "source": [
        "# Collect the exploratory data\n",
        "def exploratory(obs, **kwargs):\n",
        "    \"\"\"Adds the Gaussian noise to the expert actions.\"\"\"\n",
        "    ...\n",
        "    return action\n",
        "\n",
        "expl_data = run_policy(env, functools.partial(exploratory, cfg=cfg, actor_critic=expert, env=env, env_info=env_info, device=device), total_steps=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMgZIS9piuf5",
        "outputId": "624b2693-ae4d-4903-ee4d-7b27e7025349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.016396241262555122\n",
            "Epoch 2/10, Loss: 0.027394423261284828\n",
            "Epoch 3/10, Loss: 0.012382512912154198\n",
            "Epoch 4/10, Loss: 0.011884797364473343\n",
            "Epoch 5/10, Loss: 0.011303437873721123\n",
            "Epoch 6/10, Loss: 0.009989911690354347\n",
            "Epoch 7/10, Loss: 0.00975151639431715\n",
            "Epoch 8/10, Loss: 0.009488686919212341\n",
            "Epoch 9/10, Loss: 0.01032942347228527\n",
            "Epoch 10/10, Loss: 0.008678617887198925\n"
          ]
        }
      ],
      "source": [
        "obs_expl, act_expl, rewards, dones = expl_data\n",
        "# Exercise: Run BC on the exploratory data\n",
        "\n",
        "# ANSWER\n",
        "...\n",
        "# END ANSWER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bgu4_EG1ivJp",
        "outputId": "2098b441-224c-497e-8de0-bea1717bedb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num. episodes: 54\n",
            "Avg. return: 4446.122508227501\n",
            "Max. return: 5606.291213523451\n",
            "Min. return: 124.37031972278783\n"
          ]
        }
      ],
      "source": [
        "evaluate_agent(env, model_expl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nif8vM9xiv1I"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Answer the questions\n",
        "\n",
        "1. Why does it better?\n",
        "\n",
        "2. How can we use the expert to further improve the data?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkJSTo7CiwkJ",
        "outputId": "e719f047-1ddf-46c8-8eaf-e9c6b6616c2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.016244133934378624\n",
            "Epoch 2/10, Loss: 0.011613224633038044\n",
            "Epoch 3/10, Loss: 0.012513170950114727\n",
            "Epoch 4/10, Loss: 0.011456134729087353\n",
            "Epoch 5/10, Loss: 0.007960868999361992\n",
            "Epoch 6/10, Loss: 0.010249370709061623\n",
            "Epoch 7/10, Loss: 0.011152423918247223\n",
            "Epoch 8/10, Loss: 0.0086353225633502\n",
            "Epoch 9/10, Loss: 0.008648401126265526\n",
            "Epoch 10/10, Loss: 0.011051847599446774\n"
          ]
        }
      ],
      "source": [
        "# Exercise: Infere the expert actions on the exploratory observations\n",
        "#           and run BC on it.\n",
        "\n",
        "# ANSWER\n",
        "...\n",
        "# ANSWER END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpiX-6KxixSp",
        "outputId": "0368885f-1f29-4987-d271-15cca6efcbc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num. episodes: 102\n",
            "Avg. return: 1534.5132216604345\n",
            "Max. return: 5468.810280809093\n",
            "Min. return: 115.43894478452421\n"
          ]
        }
      ],
      "source": [
        "evaluate_agent(env, model_expl2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81NLiQFqiyHd"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Answer the questions\n",
        "\n",
        "1. Did it help? Why?\n",
        "\n",
        "\n",
        "1. How can you extend this idea?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiOVC31Qiy8q"
      },
      "source": [
        "## 2. Imitation Learning via Interactive Demostrator\n",
        "\n",
        "[DAgger](https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf)\n",
        "\n",
        "1. Collect the expert data.\n",
        "2. Fit the model (classifier/regressor) to the expert data.\n",
        "3. Collect the imitator data.\n",
        "4. Infere the expert actions on the imitator data.\n",
        "5. Fit the model to the extended dataset.\n",
        "6. Repeat from 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvy85dEoizye",
        "outputId": "0dc44432-9ebd-4643-d49d-5bcc13acdaa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.02567135915160179\n",
            "Epoch 2/10, Loss: 0.021934062242507935\n",
            "Epoch 3/10, Loss: 0.01393201481550932\n",
            "Epoch 4/10, Loss: 0.010927984490990639\n",
            "Epoch 5/10, Loss: 0.01473385002464056\n",
            "Epoch 6/10, Loss: 0.023166796192526817\n",
            "Epoch 7/10, Loss: 0.011995591223239899\n",
            "Epoch 8/10, Loss: 0.010540708899497986\n",
            "Epoch 9/10, Loss: 0.014523297548294067\n",
            "Epoch 10/10, Loss: 0.009346766397356987\n",
            "Num. episodes: 56\n",
            "Avg. return: 3017.7322707465974\n",
            "Max. return: 4360.509766970099\n",
            "Min. return: 179.2149749492237\n"
          ]
        }
      ],
      "source": [
        "# We will pre-train on less expert data to keep the same dataset size\n",
        "obs_ = obs[:2000,:]\n",
        "act_ = act[:2000,:]\n",
        "\n",
        "# EXERCISE: pretrain on first 2000 samples\n",
        "# ANSWER\n",
        "...\n",
        "# END ANSWER\n",
        "\n",
        "evaluate_agent(env, model_dagger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz_AQ9t3i0r3",
        "outputId": "df004be8-4092-4d7b-80d4-591021a188a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "### Iter. 1 ###\n",
            "\n",
            "1. Data collection\n",
            "Step 1000/2000, Steps per second: 3294.7693632223873\n",
            "Step 2000/2000, Steps per second: 3707.6622117245215\n",
            "\n",
            "2. Training\n",
            "Epoch 1/10, Loss: 0.01619561016559601\n",
            "Epoch 2/10, Loss: 0.01852767914533615\n",
            "Epoch 3/10, Loss: 0.013723315671086311\n",
            "Epoch 4/10, Loss: 0.014866928569972515\n",
            "Epoch 5/10, Loss: 0.014802966266870499\n",
            "Epoch 6/10, Loss: 0.011287961155176163\n",
            "Epoch 7/10, Loss: 0.015429697930812836\n",
            "Epoch 8/10, Loss: 0.01600058376789093\n",
            "Epoch 9/10, Loss: 0.012179029174149036\n",
            "Epoch 10/10, Loss: 0.011647832579910755\n",
            "\n",
            "3. Evaluation\n",
            "Num. episodes: 53\n",
            "Avg. return: 4466.662482786521\n",
            "Max. return: 5459.026620695091\n",
            "Min. return: 384.4503018264944\n",
            "\n",
            "### Iter. 2 ###\n",
            "\n",
            "1. Data collection\n",
            "Step 1000/2000, Steps per second: 3451.289655274561\n",
            "Step 2000/2000, Steps per second: 3548.693703259671\n",
            "\n",
            "2. Training\n",
            "Epoch 1/10, Loss: 0.01004914939403534\n",
            "Epoch 2/10, Loss: 0.010697782039642334\n",
            "Epoch 3/10, Loss: 0.009375996887683868\n",
            "Epoch 4/10, Loss: 0.00921697448939085\n",
            "Epoch 5/10, Loss: 0.009970655664801598\n",
            "Epoch 6/10, Loss: 0.01175720989704132\n",
            "Epoch 7/10, Loss: 0.010534362867474556\n",
            "Epoch 8/10, Loss: 0.007622719742357731\n",
            "Epoch 9/10, Loss: 0.01044491957873106\n",
            "Epoch 10/10, Loss: 0.010127197951078415\n",
            "\n",
            "3. Evaluation\n",
            "Num. episodes: 57\n",
            "Avg. return: 4768.642094900372\n",
            "Max. return: 6164.805995355135\n",
            "Min. return: 70.24710121710086\n",
            "\n",
            "### Iter. 3 ###\n",
            "\n",
            "1. Data collection\n",
            "Step 1000/2000, Steps per second: 3666.737478352168\n",
            "Step 2000/2000, Steps per second: 3622.9127005241353\n",
            "\n",
            "2. Training\n",
            "Epoch 1/10, Loss: 0.007917053997516632\n",
            "Epoch 2/10, Loss: 0.009996404871344566\n",
            "Epoch 3/10, Loss: 0.009498683735728264\n",
            "Epoch 4/10, Loss: 0.008137105032801628\n",
            "Epoch 5/10, Loss: 0.009362755343317986\n",
            "Epoch 6/10, Loss: 0.008086050860583782\n",
            "Epoch 7/10, Loss: 0.010338788852095604\n",
            "Epoch 8/10, Loss: 0.007983406074345112\n",
            "Epoch 9/10, Loss: 0.007980067282915115\n",
            "Epoch 10/10, Loss: 0.009110121987760067\n",
            "\n",
            "3. Evaluation\n",
            "Num. episodes: 60\n",
            "Avg. return: 4500.10965494311\n",
            "Max. return: 6043.150063411229\n",
            "Min. return: 349.27362888259813\n",
            "\n",
            "### Iter. 4 ###\n",
            "\n",
            "1. Data collection\n",
            "Step 1000/2000, Steps per second: 3578.4218204776002\n",
            "Step 2000/2000, Steps per second: 3619.2987663866243\n",
            "\n",
            "2. Training\n",
            "Epoch 1/10, Loss: 0.007250199094414711\n",
            "Epoch 2/10, Loss: 0.008454334922134876\n",
            "Epoch 3/10, Loss: 0.009125428274273872\n",
            "Epoch 4/10, Loss: 0.008615853264927864\n",
            "Epoch 5/10, Loss: 0.007782396860420704\n",
            "Epoch 6/10, Loss: 0.008323581889271736\n",
            "Epoch 7/10, Loss: 0.007024532184004784\n",
            "Epoch 8/10, Loss: 0.0071644289419054985\n",
            "Epoch 9/10, Loss: 0.009446516633033752\n",
            "Epoch 10/10, Loss: 0.007878946140408516\n",
            "\n",
            "3. Evaluation\n",
            "Num. episodes: 55\n",
            "Avg. return: 5022.044324547806\n",
            "Max. return: 5987.444502523242\n",
            "Min. return: 106.32973088577273\n"
          ]
        }
      ],
      "source": [
        "# Exercise: Implement DAgger\n",
        "\n",
        "for i in range(4):\n",
        "    print(f'\\n### Iter. {i+1} ###')\n",
        "\n",
        "    # ANSWER\n",
        "    print('\\n1. Data collection')\n",
        "    obs_extra, _, _, _ = # Collect 2k steps\n",
        "\n",
        "\n",
        "    print('\\n2. Training')\n",
        "    # reset model for fair comparison\n",
        "    model_dagger = ...\n",
        "\n",
        "    # END ANSWER\n",
        "\n",
        "    print('\\n3. Evaluation')\n",
        "    evaluate_agent(env, model_dagger)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGKMEWQti3oo"
      },
      "source": [
        "### Note\n",
        "\n",
        "Training the expert with the PPO algorithm took 10M data samples (env. interactions). Here, we nearly match it with only 10k samples! Training from the expert can be much more efficient than reinforcement learning."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
