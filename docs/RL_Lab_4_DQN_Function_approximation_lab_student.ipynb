{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEayTRvYuww1"
      },
      "source": [
        "#DQN, Function Approximation, Pefrormance tricks\n",
        "\n",
        "In this lab we study the basics of Q learning with function approximation by neural networks."
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I32BwFgeVhHg",
        "outputId": "267527a1-1f07-47ea-e67d-584879889f09"
      },
      "outputs": [],
      "source": [
        "# Installing dependencies for visualization\n",
        "!apt-get -qq -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1 > /dev/null\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "!pip -q install gymnasium[classic_control]\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ckBLjgWAeQy5"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import random\n",
        "import time\n",
        "import collections\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model, clone_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from base64 import b64encode\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Start virtual display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "\n",
        "\n",
        "def show_video(file_name: str):\n",
        "    mp4 = open(file_name, \"rb\").read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    return HTML(\n",
        "        \"\"\"\n",
        "    <video width=480 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\"\n",
        "        % data_url\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvcjOL9MborZ"
      },
      "source": [
        "We will start by defining some useful data structure:"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5U1thRNbyNO",
        "outputId": "3ea07829-a84b-42a7-edb2-47b9cacc872c"
      },
      "outputs": [],
      "source": [
        "Transition = collections.namedtuple(\n",
        "    \"transition\", [\"state\", \"action\", \"reward\", \"done\", \"next_state\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPlVCdaAchgH"
      },
      "source": [
        "## CartPole\n",
        "Debugging DQN is typically a complicated process, thus we have to start with a simple environment, that can be quickly iterated. Let's first construct working DQN for CartPole problem. We will use a small modification of the orginal CartPole env, we do reward reshape (to make problem easier for DQN):"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "YP_8AWXAeFX7"
      },
      "outputs": [],
      "source": [
        "class ModifiedCartPole:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        return self.env.reset()[0]\n",
        "\n",
        "    def step(self, action) -> Tuple[np.ndarray, float, bool, Dict[Any, Any]]:\n",
        "        obs, reward, done, truncated, _ = self.env.step(action)\n",
        "        if done:\n",
        "            reward = -10\n",
        "        return obs, reward / 10, done or truncated, {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrXppGmcc6aB"
      },
      "source": [
        "##Q-network.\n",
        "First we must create a network to approximate Q(s, a). We have two natural design choices:\n",
        "- Q-network takes two inputs: state s and action a and predicts one value Q(s,a)\n",
        "- Q-network takes one input: state s, and predicts a vector of Q(s, a) for all possible actions.\n",
        "\n",
        "We will follow the second design choice (one of the reasons is that such network can faster predict the best action).\n",
        "\n",
        "**Exercise: fill the code below to create Q-network** Create a simple fully connected network with `num_layers` layers each with 64 neurons. The input is a vector of size `input_size`, and the output is a vector of size `num_actions` (we have 2 actions in cartpole)."
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "4hivxFi6BWRi"
      },
      "outputs": [],
      "source": [
        "def make_cartpole_network(\n",
        "    input_size: int = 4,\n",
        "    num_action: int = 2,\n",
        "    num_layers: int = 3,\n",
        "    learning_rate: float = 1e-4,\n",
        "    weight_decay: float = 0.0,\n",
        ") -> Model:\n",
        "    input_state = Input(batch_shape=(None, input_size))\n",
        "    #### TODO ####\n",
        "    #### END ####\n",
        "    model = Model(inputs=input_state, outputs=output)\n",
        "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=learning_rate))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIctRLzEgxNN"
      },
      "source": [
        "## Building DQN\n",
        "\n",
        "We will start with some utils functions:\n",
        "\n",
        "**Exercise: read the following functions, implement epsilon greedy policy**"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "G-7XROP_nkBm"
      },
      "outputs": [],
      "source": [
        "def predict_q_values(q_network: Model, state: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Makes a prediction for a single state and returns array of Q-values\"\"\"\n",
        "    return q_network.predict(np.array([state]), verbose=0)[0]\n",
        "\n",
        "\n",
        "def choose_best_action(q_network: Model, state: np.ndarray) -> int:\n",
        "    \"\"\"Chooses best action according to Q-network\"\"\"\n",
        "    action_values = predict_q_values(q_network, state)\n",
        "    best_action = np.argmax(action_values)\n",
        "    return best_action\n",
        "\n",
        "\n",
        "def evaluate_state_batch(\n",
        "    target_network: Model, state_batch: np.ndarray\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"This function can evaluate the whole batch of states at once, it\n",
        "    is very useful to speedup the training when we calculate targets\n",
        "    Arguments:\n",
        "      - state_batch: list of states to evaluate\n",
        "    Returns:\n",
        "      - best actions: list of best action for every state\n",
        "      - best vals: list of best state-action values for very state\n",
        "      - action_values: list of all action-values for each state\n",
        "\n",
        "    Here we named the argument target network instead of q_network, because this\n",
        "    function will be used with target network.\n",
        "    \"\"\"\n",
        "    action_values = target_network.predict(np.array(state_batch), verbose=0)\n",
        "    best_actions = np.argmax(action_values, axis=-1)\n",
        "    best_vals = np.max(action_values, axis=-1)\n",
        "    return best_actions, best_vals, action_values\n",
        "\n",
        "\n",
        "def choose_action(q_network: Model, state, epsilon: float) -> int:\n",
        "    \"\"\"Implement epsilon-greedy policy.\"\"\"\n",
        "    #### TODO ####\n",
        "    #### END ####\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRiSgvoRueyJ"
      },
      "source": [
        "While running the episodes we will collect transitions and store them in a replay_buffer, which is just a list of transitions. Before we write a code for running episodes we must first prepare a function that prepares training (since it is used while running the game) and a one for doing the training.\n",
        "\n",
        "**Exercise: the training protocole is the heart of DQN. Fill the gaps in the following function. Specific tasks are described in multiline comments.**"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ZuMdEiapC5Xp"
      },
      "outputs": [],
      "source": [
        "def sample_minibatch(\n",
        "    replay_buffer: List[Transition], mini_batch_size: int\n",
        ") -> List[Transition]:\n",
        "    \"\"\"Write a code to choose random samples from replay_buffer.\n",
        "    Choose mini_batch_size of samples and collect them in replay_batch variable.\n",
        "    replay_batch must be a list of transitions.\n",
        "    Hint: you can use random.sample method.\"\"\"\n",
        "    replay_batch: list\n",
        "    #### TODO ####\n",
        "    #### END ####\n",
        "    return replay_batch\n",
        "\n",
        "\n",
        "def compute_target(\n",
        "    transition: Transition, next_state_value: float, gamma: float\n",
        ") -> float:\n",
        "    \"\"\"Compute TD(1) target based on current transition and next state value.\n",
        "    Remember to treat last state of the episode separately.\n",
        "    \"\"\"\n",
        "    #### TODO ####\n",
        "    #### END ####\n",
        "    return target\n",
        "\n",
        "\n",
        "def prepare_update_targets(\n",
        "    target_network: Model,\n",
        "    replay_buffer: List[Transition],\n",
        "    mini_batch_size: int,\n",
        "    gamma: float = 0.99,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    replay_batch = sample_minibatch(replay_buffer, mini_batch_size)\n",
        "\n",
        "    # We will collect all states and next_states from the batch of transitions\n",
        "    # to evaluate them at once.\n",
        "    next_state_batch = [transition.next_state for transition in replay_batch]\n",
        "    state_batch = [transition.state for transition in replay_batch]\n",
        "\n",
        "    _, next_state_values, _ = evaluate_state_batch(target_network, next_state_batch)\n",
        "    _, _, state_action_vals = evaluate_state_batch(target_network, state_batch)\n",
        "\n",
        "    train_x, train_y = [], []\n",
        "    for transition, state_vals, next_state_value in zip(\n",
        "        replay_batch, state_action_vals, next_state_values\n",
        "    ):\n",
        "        \"\"\"Prepare x, y training pairs for supervised model update:\n",
        "        - x is a state,\n",
        "        - y is a vector of values for each action.\n",
        "        Note, that we only compute new targets for one action (the one in the trajectory),\n",
        "        values for other actions should remain unchanged.\n",
        "        Hint: use copy() method to make sure you are not modifying data in replay buffer.\n",
        "        \"\"\"\n",
        "        # Copy transition.state to x, use copy() method\n",
        "        action = transition.action\n",
        "        x = transition.state.copy()\n",
        "        # Copy state_vals vector to y\n",
        "        y = state_vals.copy()\n",
        "\n",
        "        y[action] = compute_target(transition, next_state_value, gamma)\n",
        "\n",
        "        train_x.append(x)\n",
        "        train_y.append(y)\n",
        "\n",
        "    return np.array(train_x), np.array(train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9XEP4-vGxK9"
      },
      "source": [
        "**Exercise: fill the gaps in the update function**"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "5CnS5l9kv3N_"
      },
      "outputs": [],
      "source": [
        "def update(\n",
        "    q_network: Model,\n",
        "    target_network: Model,\n",
        "    replay_buffer: List[Transition],\n",
        "    mini_batch_size: int,\n",
        "    gamma: float,\n",
        ") -> float:\n",
        "    \"\"\"Prepare training batch (x and y) and update the model on it.\n",
        "    For models in keras you can use train_on_batch method. Checkout its\n",
        "    interface in the documentation.\"\"\"\n",
        "    #### TODO ####\n",
        "    #### END ####\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv7eYsOj37y7"
      },
      "source": [
        "Now, let us code the heart of DQN algorithm: the function that runs an epizode and trains Q-network.\n",
        "\n",
        "**Exercise: fill the code in run_one_episode function**"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "1hBpzbmmHhY3"
      },
      "outputs": [],
      "source": [
        "def run_one_episode(\n",
        "    q_network: Model,\n",
        "    target_network: Model,\n",
        "    env: ModifiedCartPole,\n",
        "    epsilon: float,\n",
        "    steps_so_far: int,\n",
        "    replay_buffer: List[Transition],\n",
        "    mini_batch_size: int,\n",
        "    update_target_every_n_steps: int,\n",
        "    gamma: float,\n",
        ") -> Tuple[int, List[float]]:\n",
        "    done = False\n",
        "    episode_steps = 0\n",
        "    state = env.reset()\n",
        "    ep_actions = []\n",
        "    loss_history = []\n",
        "    while not done:\n",
        "        # Implement env interaction\n",
        "        # 1. Select action with eps-greedy policy\n",
        "        # 2. Advance the env\n",
        "        # 3. Store recorded transition in the replay buffer\n",
        "        # Remember to update the `state` variable\n",
        "        ### TODO ###\n",
        "        ### END ###\n",
        "        ep_actions.append(action)\n",
        "        episode_steps += 1\n",
        "        steps_so_far += 1\n",
        "\n",
        "        if len(replay_buffer) > mini_batch_size:\n",
        "            # Update the model\n",
        "            ### TODO ###\n",
        "            ### END ###\n",
        "            loss_history.append(loss)\n",
        "            if steps_so_far % update_target_every_n_steps == 0:\n",
        "                print(f\"Updating target network\")\n",
        "                target_network.set_weights(q_network.get_weights())\n",
        "\n",
        "    return episode_steps, loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdYqQU9l1a8X"
      },
      "source": [
        "Finally, we can complete the full DQN algorithm."
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "T9L7S4yMJhy0"
      },
      "outputs": [],
      "source": [
        "def run_dqn(\n",
        "    train_steps: int, n_checkpoints: int\n",
        ") -> Tuple[List[int], List[float], List[np.ndarray]]:\n",
        "    # The parameter\n",
        "    env = ModifiedCartPole()\n",
        "\n",
        "    # We save several checkpoints to later visualize theit performance\n",
        "    q_checkpoints = []\n",
        "    save_q_chepoint_every_n_steps = train_steps / n_checkpoints\n",
        "\n",
        "    # Here is a set of default parameters (tested), you can try to find better values\n",
        "    epsilon = 0.4\n",
        "    min_epsilon = 0.1\n",
        "    epsilon_decay = 0.99\n",
        "    gamma = 0.975\n",
        "    mini_batch_size = 128\n",
        "    update_target_every_n_steps = 128\n",
        "\n",
        "    replay_buffer = []\n",
        "\n",
        "    q_network = make_cartpole_network()\n",
        "    target_network = make_cartpole_network()\n",
        "\n",
        "    steps_so_far = 0\n",
        "\n",
        "    episode_lengths, loss_history = [], []\n",
        "    episode_num = 0\n",
        "\n",
        "    while steps_so_far < train_steps:\n",
        "        episode_length, loss = run_one_episode(\n",
        "            q_network,\n",
        "            target_network,\n",
        "            env,\n",
        "            epsilon,\n",
        "            steps_so_far,\n",
        "            replay_buffer,\n",
        "            mini_batch_size,\n",
        "            update_target_every_n_steps,\n",
        "            gamma,\n",
        "        )\n",
        "        if epsilon > min_epsilon:\n",
        "            epsilon *= epsilon_decay\n",
        "        episode_num += 1\n",
        "        episode_lengths.append(episode_length)\n",
        "        if loss is not None:\n",
        "            loss_history.extend(loss)\n",
        "        steps_so_far += episode_length\n",
        "        if (\n",
        "            steps_so_far - len(q_checkpoints) * save_q_chepoint_every_n_steps\n",
        "            >= save_q_chepoint_every_n_steps\n",
        "        ):\n",
        "            q_checkpoints.append(q_network.get_weights())\n",
        "        print(\n",
        "            f\"Episode = {episode_num} | steps =  {steps_so_far} | \"\n",
        "            f\"episode_length = {episode_length} | epsilon = {epsilon} | \"\n",
        "            f\"loss = {np.mean(loss)}\"\n",
        "        )\n",
        "\n",
        "    return episode_lengths, loss_history, q_checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3_LzNLP464R"
      },
      "source": [
        "Let us now run the training (it may take several minutes to take the training of 5000-8000 steps). Do not expect the reward to grow monotonically. The training typically looks like a noisy process with some drift towards higher returns."
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOKTRkSS490q",
        "outputId": "55c384d4-2f9e-47e8-c43d-e3777a2e19ce"
      },
      "outputs": [],
      "source": [
        "progress, loss_history, q_checkpoints = run_dqn(2000, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "NUkrjbY41kxF"
      },
      "outputs": [],
      "source": [
        "def visualize_progress(progress: List[int], loss_history: List[float]):\n",
        "    plt.clf()\n",
        "    plt.plot(progress, label=\"DQN progress\")\n",
        "    smoothed_progress = [0]\n",
        "    for x in progress:\n",
        "        smoothed_progress.append(0.8 * smoothed_progress[-1] + 0.2 * x)\n",
        "    plt.plot(smoothed_progress, label=\"DQN learning (smoothed)\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.clf()\n",
        "    plt.plot(loss_history, label=\"Loss\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "kovVfKlO6CrJ",
        "outputId": "f34eb7c0-cdbe-4cd1-9c4e-a2719b1fee0c"
      },
      "outputs": [],
      "source": [
        "visualize_progress(progress, loss_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VokUnDv0GszE"
      },
      "source": [
        "Let us see how the agent performs across the training:"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "1K0kLsJ7hN6h"
      },
      "outputs": [],
      "source": [
        "def record_checkpoint(checkpoint: np.ndarray):\n",
        "    # This function records an episode of the agent equipped with a given chekpoint\n",
        "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "    model = make_cartpole_network()\n",
        "    model.set_weights(checkpoint)\n",
        "    max_ep_len = 200\n",
        "    envw = gym.wrappers.RecordVideo(env, \"./\", name_prefix=\"cartpole-video\")\n",
        "    (o, info_), d, ep_len = envw.reset(), False, 0\n",
        "    while not (d or (ep_len == max_ep_len)):\n",
        "        envw.render()\n",
        "        action = choose_best_action(model, o)\n",
        "        o, r, d, t, info = envw.step(action)\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmLU4eEgIxno"
      },
      "source": [
        "Lets take a look at first saved chekpoint:"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "id": "zTkCCWwwjqc-",
        "outputId": "4db5a800-7530-48ba-be89-adc2926c01d4"
      },
      "outputs": [],
      "source": [
        "record_checkpoint(q_checkpoints[0])\n",
        "file_name = glob.glob(\"cartpole-video*.mp4\")[0]\n",
        "show_video(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeN9OPMVJBsX"
      },
      "source": [
        "And the last:"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TcrqaiHuJA2O",
        "outputId": "7d8fad54-e92b-4f3e-c83d-c47f82d9c607"
      },
      "outputs": [],
      "source": [
        "record_checkpoint(q_checkpoints[-1])\n",
        "file_name = glob.glob(\"cartpole-video*.mp4\")[0]\n",
        "show_video(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edjScjA4KLJJ"
      },
      "source": [
        "# Ablation study\n",
        "Let's see the what happens to DQN performance after turning off some of its mechanisms:\n",
        "- target network\n",
        "- sampling from replay_buffer\n",
        "\n",
        "**Exercise: turn off the usage of target networks.** You can for example modify the code of run_dqn() and set target_network = q_network. Compare the results with previous run.\n",
        "\n",
        "**Exercise: add the size limit to replay buffer.** Add a code to run_dqn() that clips its size to a given limit. What happens if the replay buffer is very small?"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXibVyhoor_Y"
      },
      "source": [],
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}