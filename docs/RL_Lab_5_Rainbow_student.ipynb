{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "91a0c19f",
      "metadata": {
        "id": "91a0c19f"
      },
      "source": [
        "# Sample Efficient Reinforcement Learning - from DQN to (almost) Rainbow\n",
        "\n",
        "In this scenario we will expand upon on the Deep Q-Network (DQN) algorithm [(Mnih 2014)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf). DQN has been successfully applied to a wide range of environments and has demonstrated strong performance on many tasks. However, several challenges and limitations to the DQN that have been identified in the literature:\n",
        "\n",
        "1. Sample complexity - DQN can require a large number of samples to learn effectively, especially in environments with high-dimensional state spaces or a large number of possible actions\n",
        "2. Convergence - DQN is known to converge to the optimal solution under certain conditions, but the convergence properties of the algorithm are not well understood and it is not guaranteed to converge in all cases\n",
        "3. Overestimation - DQN is known to sometimes overestimate the Q-values of certain actions, which can lead to suboptimal behavior\n",
        "4. Sensitivity to hyperparameters - DQN can be sensitive to the choice of hyperparameters, such as the learning rate, the discount factor, and the exploration scheme.\n",
        "\n",
        "Rainbow algorithm [(Hessel 2017)](https://arxiv.org/pdf/1710.02298.pdf) is a combination of several techniques for improving the performance of the DQN algorithm, which was originally proposed by DeepMind. By combining several techniques, the Rainbow algorithm is able to improve the sample efficiency, stability and  performance of the DQN algorithm. Overall, the Rainbow algorithm represents an important step forward in the development of reinforcement learning algorithms and is often used as a baseline for implementing more complex changes to the RL setup (e.g. [(Schwarzer 2021)](https://arxiv.org/pdf/2007.05929.pdf) or [(Srinivas 2020)](https://arxiv.org/pdf/2004.04136.pdf))\n",
        "\n",
        "In this scenario, you will learn to augment a simple DQN implementation with some of the components of Rainbow. To test our implementations, we will use the Lunar Lander environment. Given resources, the environment is easily solved by a vanilla DQN implementation. But we do not have resources. What we have is a budget of:\n",
        "\n",
        "1. 40 000 environment steps\n",
        "2. 35 000 Q-network updates \n",
        "\n",
        "And quite inefficient exploration scheme. As such, our basic DQN implementation will not be enough to solve Lunar Landing problem within the constraints.\n",
        "\n",
        "## Homework scenario and grading\n",
        "\n",
        "We provide you with a basic implementation of the DQN. Your job is to expand it with the following modules:\n",
        "\n",
        "1. Double Q-Learning - [(van Hasselt 2015)](https://arxiv.org/pdf/1509.06461.pdf) \n",
        "2. N-step learning - [(Sutton 1988)](http://incompleteideas.net/papers/sutton-88-with-erratum.pdf)\n",
        "3. Dueling network architecture - [(Wang et al. 2015)](https://arxiv.org/pdf/1511.06581.pdf)\n",
        "4. (Almost) Rainbow - [(Hessel 2017)](https://arxiv.org/pdf/1710.02298.pdf) \n",
        "\n",
        "Each module is designed to work independently (i.e. you can implement each individually with DQN). The final task of this scenario is to combine all the implemented modules into (almost) Rainbow agent. Don't forget to visualize your results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "949c29dc",
      "metadata": {
        "id": "949c29dc"
      },
      "source": [
        "We import the necessary modules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z_2HKKYIdygC",
      "metadata": {
        "id": "Z_2HKKYIdygC"
      },
      "outputs": [],
      "source": [
        "!pip install swing\n",
        "!pip install box2d\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0728d82",
      "metadata": {
        "id": "d0728d82"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "import gymnasium as gym\n",
        "# from google.colab import files\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "094e4c1f",
      "metadata": {
        "id": "094e4c1f"
      },
      "source": [
        "We define a simple class for holding the hyperparameters (do not change those!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1883162",
      "metadata": {
        "id": "e1883162"
      },
      "outputs": [],
      "source": [
        "# do not change!\n",
        "class parse_args:\n",
        "    def __init__(self):\n",
        "        self.gym_id = \"LunarLander-v2\"\n",
        "        self.capacity = 10000\n",
        "        self.init_steps = 10000\n",
        "        self.batch_size = 128\n",
        "        self.hidden_dim = 128\n",
        "        self.learning_rate = 7e-4\n",
        "        self.discount = 0.99\n",
        "        self.samples = 3\n",
        "        self.total_timesteps = 40000\n",
        "        self.target_update_freq = 50\n",
        "        self.evaluate_freq = 1000\n",
        "        self.evaluate_samples = 5\n",
        "        self.anneal_steps = 30000\n",
        "        self.epsilon_limit = 0.01\n",
        "        self.cuda = True\n",
        "        env = gym.make(self.gym_id)\n",
        "        self.state_dim = env.observation_space.shape[0]\n",
        "        self.action_dim = env.action_space.n\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and self.cuda else \"cpu\")\n",
        "        \n",
        "args = parse_args()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0866057",
      "metadata": {
        "id": "e0866057"
      },
      "source": [
        "And two helper functions: one for setting seeds, one for simple orthogonal initialization of linear layers, and one for saving and downloading training results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97603d29",
      "metadata": {
        "id": "97603d29"
      },
      "outputs": [],
      "source": [
        "def set_seed_everywhere(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "def weight_init(model):\n",
        "    if isinstance(model, nn.Linear):\n",
        "        nn.init.orthogonal_(model.weight.data)\n",
        "        model.bias.data.fill_(0.0)\n",
        "\n",
        "def download_numpy(filename, data):\n",
        "    np.save(filename, data)\n",
        "    files.download(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32176d86",
      "metadata": {
        "id": "32176d86"
      },
      "source": [
        "## 0. DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ed14f18",
      "metadata": {
        "id": "8ed14f18"
      },
      "source": [
        "Deep Q-Network (DQN) [(Mnih 2014)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) is a reinforcement learning algorithm that uses a deep neural network to learn a Q-function, which is a function that estimates the expected return for taking a given action in a given state. The goal of the DQN algorithm is to learn a policy that maximizes the expected return by learning the Q-function and selecting the action with the highest estimated return in each state.\n",
        "\n",
        "The DQN algorithm consists of two main components: a Q-network and an experience buffer. The Q-network is a deep neural network that takes in a state as input and outputs the estimated Q-values for each possible action. The experience buffer is a data structure that stores a set of experiences. The DQN algorithm works by interacting with the environment and storing the experiences in the experience buffer. The Q-network is then trained using a mini-batch of experiences uniformly sampled from the experience buffer. This process is known as experience replay and is used to decorrelate the experiences and to stabilize the learning process. The Q-network is updated using the loss function:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\theta} = \\frac{1}{B} \\sum_{i=1}^{B} \\bigl( \\mathrm{TD}~(s_i, a_i, s^{'}_{i}) \\bigr)^{2}\n",
        "$$\n",
        "\n",
        "With:\n",
        "\n",
        "$$\n",
        "\\mathrm{TD}~(s_i, a_i, s^{'}_{i}) = Q_{\\theta}~(s_i,a_i) - \\bigl(r_{(s_i,a_i,s_{i}^{'})} + \\gamma ~ \\underset{a^{'}_{i} \\sim \\bar{Q}_{\\theta}}{\\mathrm{max}} ~ \\bar{Q}_{\\theta}~(s_{i}^{'},a_{i}^{'}) \\bigr)\n",
        "$$\n",
        "\n",
        "Where $Q_{\\theta}$ and $\\bar{Q}_{\\theta}$ denote learned and target Q-networks respectively. The target network is a copy of the Q-network that is updated less frequently, and using it to compute the target Q-values helps to stabilize the learning process and improve the performance of the DQN algorithm. Note that to increase stability of training we use Huber loss (smooth_l1_loss) instead of L2.\n",
        "\n",
        "There are several ways to incorporate exploration into the DQN algorithm. One common method is to use an $\\epsilon$-greedy exploration strategy, where the agent takes a random action with probability $\\epsilon$ and takes the action with the highest estimated Q-value with probability $1 - \\epsilon$. The value of $\\epsilon$ is typically decreased over time, so that the agent initially explores more and then gradually shifts towards exploitation as it learns more about the environment.\n",
        "\n",
        "Below, we implement all the components of a basic DQN. We start with the experience buffer - a data structure that stores a set of transitions, where a transition is typically represented as a tuple $(s, a, r, s', t)$, where $s$ is the state, $a$ is the action taken in state $s$, $r$ is the reward received by performing $a$ in $s$ and getting to $s'$, $s'$ is the new state observed after performing $a$ in $s$ and $t$ is the termination boolean (true if $s'$ is terminal). Experience buffers are used to store the experiences of an agent as it interacts with an environment, and are used to train a Q-function, which is a function that estimates the expected return for taking a given action in a given state. We implement **ExperienceBuffer** class using NumPy arrays and we define two methods:\n",
        "\n",
        "1. *add* - adds transition to the buffer\n",
        "2. *sample* - samples a batch of transitions from the buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53e147c5",
      "metadata": {
        "id": "53e147c5"
      },
      "outputs": [],
      "source": [
        "class ExperienceBuffer:\n",
        "    def __init__(self, args):\n",
        "        self.states = np.zeros((args.capacity, args.state_dim), dtype=np.float32)\n",
        "        self.actions = np.zeros((args.capacity, 1), dtype=np.int64)\n",
        "        self.rewards = np.zeros((args.capacity, 1), dtype=np.float32)\n",
        "        self.next_states = np.zeros((args.capacity, args.state_dim), dtype=np.float32)\n",
        "        self.terminals = np.zeros((args.capacity, 1), dtype=np.int64)\n",
        "        self.full = False\n",
        "        self.idx = 0\n",
        "        self.args = args \n",
        "        \n",
        "    def add(self, state, action, reward, next_state, terminal):\n",
        "        self.states[self.idx, :] = state\n",
        "        self.actions[self.idx, :] = action\n",
        "        self.rewards[self.idx, :] = reward\n",
        "        self.next_states[self.idx, :] = next_state\n",
        "        self.terminals[self.idx, :] = 1 if terminal else 0\n",
        "        self.idx += 1\n",
        "        if self.idx == self.args.capacity:\n",
        "            self.full = True\n",
        "            self.idx = 0\n",
        "            \n",
        "    def sample(self):\n",
        "        idx = np.random.permutation(self.args.capacity)[:self.args.batch_size] if self.full else np.random.permutation(self.idx-1)[:self.args.batch_size]\n",
        "        states = torch.from_numpy(self.states[idx]).to(self.args.device)\n",
        "        actions = torch.from_numpy(self.actions[idx]).to(self.args.device)\n",
        "        rewards = torch.from_numpy(self.rewards[idx]).to(self.args.device)\n",
        "        next_states = torch.from_numpy(self.next_states[idx]).to(self.args.device)\n",
        "        terminals = torch.from_numpy(self.terminals[idx]).long().to(self.args.device)\n",
        "        return states, actions, rewards, next_states, terminals"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fdcd965",
      "metadata": {
        "id": "9fdcd965"
      },
      "source": [
        "**QNetwork** class is a simple nn.Module MLP. Note the output size being equal to the amount of actions in the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3fb7c14",
      "metadata": {
        "id": "e3fb7c14"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "           nn.Linear(args.state_dim, args.hidden_dim), nn.ReLU(),\n",
        "            nn.Linear(args.hidden_dim, args.hidden_dim), nn.ReLU(),\n",
        "            nn.Linear(args.hidden_dim, args.action_dim))\n",
        "        self.apply(weight_init)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ed7a71",
      "metadata": {
        "id": "03ed7a71"
      },
      "source": [
        "Finally we implement DQN agent. The class has following methods:\n",
        "\n",
        "1. *get_action* - returns action in given state using $\\epsilon$-greedy\n",
        "2. *anneal* - reduces the value of $\\epsilon$ dependent on the training step\n",
        "3. *update* - samples a batch of transitions from the experience buffer and performs a DQN update\n",
        "4. *update_target* - performs a hard update on the target Q network $\\bar{Q}_{\\theta}$\n",
        "5. *evaluate* - performs evaluation of the agent with a greedy policy \n",
        "6. *reset* - resets the agent (used between seeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28b71037",
      "metadata": {
        "id": "28b71037"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "    def __init__(self, args):\n",
        "        super(DQN, self).__init__()\n",
        "        self.args = args \n",
        "        self.buffer = ExperienceBuffer(self.args)\n",
        "        self.epsilon = 1\n",
        "        self.q_net = QNetwork(self.args).to(self.args.device)\n",
        "        self.q_target = QNetwork(self.args).to(self.args.device)\n",
        "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
        "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=self.args.learning_rate, eps=1e-5)\n",
        "                \n",
        "    def get_action(self, state, exploration=True):\n",
        "        with torch.no_grad():\n",
        "            return np.random.randint(self.args.action_dim) if np.random.sample() < self.epsilon and exploration else torch.argmax(self.q_net(state)).item()\n",
        "\n",
        "    def anneal(self, step):\n",
        "        self.epsilon = ((self.args.epsilon_limit - 1)/self.args.anneal_steps) * step + 1 if step < self.args.anneal_steps else self.epsilon\n",
        "\n",
        "    def update(self):\n",
        "        states, actions, rewards, next_states, terminals = self.buffer.sample()\n",
        "        with torch.no_grad():\n",
        "            q_ns = torch.max(self.q_target(next_states), dim=1)[0].unsqueeze(1)\n",
        "        q_targets = rewards + (1-terminals) * self.args.discount * q_ns\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        q_values = self.q_net(states).gather(1, actions)\n",
        "        loss = nn.functional.smooth_l1_loss(q_values, q_targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "    \n",
        "    def update_target(self):\n",
        "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
        "        \n",
        "    def evaluate(self, samples):\n",
        "        with torch.no_grad():\n",
        "            env_test = gym.make(self.args.gym_id)\n",
        "            eval_reward = 0\n",
        "            for i in range(samples):\n",
        "                state, info_ = env_test.reset()\n",
        "                episode_reward = 0\n",
        "                while True:\n",
        "                    action = self.get_action(torch.tensor(state).unsqueeze(0).to(self.args.device), False)\n",
        "                    next_state, reward, terminal, truncated, _ = env_test.step(action)\n",
        "                    episode_reward += reward\n",
        "                    state = next_state\n",
        "                    if terminal or truncated:\n",
        "                        eval_reward += episode_reward/samples\n",
        "                        break\n",
        "        return eval_reward\n",
        "    \n",
        "    def reset(self):\n",
        "        self.buffer = ExperienceBuffer(self.args)\n",
        "        self.epsilon = 1\n",
        "        self.q_net = QNetwork(self.args).to(self.args.device)\n",
        "        self.q_target = QNetwork(self.args).to(self.args.device)\n",
        "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
        "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=self.args.learning_rate, eps=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f539a644",
      "metadata": {
        "id": "f539a644"
      },
      "source": [
        "Finally, we provide code for agent training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "385a9330",
      "metadata": {
        "id": "385a9330"
      },
      "outputs": [],
      "source": [
        "def train_agent(args, agent):\n",
        "    results = np.zeros((args.total_timesteps//args.evaluate_freq, args.samples))\n",
        "    for seed in range(args.samples):\n",
        "        env = gym.make(args.gym_id)\n",
        "        agent.reset()\n",
        "        set_seed_everywhere(seed)\n",
        "        state, info_ = env.reset(seed=seed)\n",
        "        for step in range(args.total_timesteps):\n",
        "            if step == args.init_steps:\n",
        "                start_time = time.time()\n",
        "            action = agent.get_action(torch.tensor(state).unsqueeze(0).to(args.device))\n",
        "            next_state, reward, terminal, truncated, _ = env.step(action)\n",
        "            agent.buffer.add(state, action, reward, next_state, terminal)\n",
        "            agent.anneal(step)\n",
        "            state = next_state\n",
        "            if step >= args.init_steps:\n",
        "                agent.update()\n",
        "                if (step + 1) % args.target_update_freq == 0:\n",
        "                    agent.update_target()\n",
        "                if (step + 1) % args.evaluate_freq == 0:\n",
        "                    eval_reward = agent.evaluate(args.evaluate_samples)\n",
        "                    results[step//args.evaluate_freq, seed] = eval_reward\n",
        "                    print(\"\\rStep: {} Evaluation reward: {:.2f} Samples per second: {:} \\t \\t\".format(step, eval_reward, int((step-args.init_steps)/(time.time()-start_time))), end=\"\")\n",
        "            if terminal or truncated:\n",
        "                state, info_ = env.reset()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff186311",
      "metadata": {
        "id": "ff186311"
      },
      "source": [
        "Note that you should not change the code above - you should be able to perform all tasks by creating new classes. We train the DQN agent with given hyperparameters and inspect the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938d29b1",
      "metadata": {
        "id": "938d29b1"
      },
      "outputs": [],
      "source": [
        "agent = DQN(args)\n",
        "results_dqn = train_agent(args, agent)\n",
        "download_numpy(\"results_dqn.npy\", results_dqn)\n",
        "results_dqn.mean(1)[-10:].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3376e3e2",
      "metadata": {
        "id": "3376e3e2"
      },
      "source": [
        "As you can see, the vanilla DQN does not yield optimal performance given the budget and exploration constraints. Below is the first module that you have to add to the DQN algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64e5bfc1",
      "metadata": {
        "id": "64e5bfc1"
      },
      "source": [
        "## 1. Double DQN\n",
        "\n",
        "The loss function of vanilla DQN is defined as the average of single transition temporal difference (TD) error over $B$ transitions:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\theta} = \\frac{1}{B} \\sum_{i=1}^{B} \\bigl( \\mathrm{TD}~(s_i, a_i, s^{'}_{i}) \\bigr)^{2}\n",
        "$$\n",
        "\n",
        "With transitions $(s_i, a_i, s^{'}_{i})$ sampled uniformly from the experience buffer. The transition TD error is defined through Bellman optimality condition:\n",
        "\n",
        "$$\n",
        "\\mathrm{TD}~(s_i, a_i, s^{'}_{i}) = Q_{\\theta}~(s_i,a_i) - \\bigl(r_{(s_i,a_i,s_{i}^{'})} + \\gamma ~ \\underset{a^{'}_{i} \\sim \\bar{Q}_{\\theta}}{\\mathrm{max}} ~ \\bar{Q}_{\\theta}~(s_{i}^{'},a_{i}^{'}) \\bigr)\n",
        "$$\n",
        "\n",
        "Where $Q_{\\theta}$ and $\\bar{Q}_{\\theta}$ denote learned and target Q-networks respectively. In the setup above $a_{i}^{'}$ is chosen via maximum operation over the output of the target Q-network for $s^{'}_{i}$. Using a single network to choose the best action and estimate its Q-value promotes overestimated values. Using such values for supervision leads in turn to general overoptimism of the Q-network and is known to sabotage the training.\n",
        "\n",
        "In Double Deep Q-Network (DDQN) [(van Hasselt 2015)](https://arxiv.org/pdf/1509.06461.pdf) proposes using two Q-networks in the process of target estimation: one Q-network to choose the maximum valued action from (i.e. *argmax*); and the second one to estimate value of the chosen action (i.e. Q-value estimation for the *argmax* result). Authors show that in DDQN estimated Q-values are less likely to be inflated and lead to more stable learning and better policies. We can use $Q_{\\theta}$ and $\\bar{Q}_{\\theta}$ to augment DQN into DDQN: \n",
        "\n",
        "$$\n",
        "\\mathrm{TD}~(s_i, a_i, s^{'}_{i}) = Q_{\\theta}~(s_i,a_i) - \\bigl(r_{(s_i,a_i,s_{i}^{'})} + \\gamma ~ \\bar{Q}_{\\theta}~(s_{i}^{'},\\underset{a^{'}_{i} \\sim Q_{\\theta}}{\\mathrm{argmax}} ~ Q_{\\theta} (s_{i}^{'}, a^{'}_{i})  \\bigr)\n",
        "$$\n",
        "\n",
        "Such definition of DDQN leads to very small code changes w.r.t. vanilla DQN implementation. Although $Q_{\\theta}$ and $\\bar{Q}_{\\theta}$ are not fully decoupled, using them leads to good performance increase without introduction of additional networks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19bee972",
      "metadata": {
        "id": "19bee972"
      },
      "source": [
        "### Task 1.1: Implement and train DDQN \n",
        "Implement the *update* method for **DDQN** class (no other method of the base class should be changed): "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b682c0",
      "metadata": {
        "id": "c7b682c0"
      },
      "outputs": [],
      "source": [
        "class DDQN(DQN):\n",
        "    def __init__(self, args):\n",
        "        super(DDQN, self).__init__(args)\n",
        "        \n",
        "    def update(self):\n",
        "        states, actions, rewards, next_states, terminals = self.buffer.sample()\n",
        "        ####### TODO ########\n",
        "        # q_targets = \n",
        "        ####### END ########\n",
        "        self.optimizer.zero_grad()\n",
        "        q_values = self.q_net(states).gather(1, actions)\n",
        "        loss = nn.functional.smooth_l1_loss(q_values, q_targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "926c3cc9",
      "metadata": {
        "id": "926c3cc9"
      },
      "outputs": [],
      "source": [
        "agent = DDQN(args)\n",
        "results_dqn1 = train_agent(args, agent)\n",
        "download_numpy(\"results_dqn1.npy\", results_dqn1)\n",
        "results_dqn1.mean(1)[-10:].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "004bf3a3",
      "metadata": {
        "id": "004bf3a3"
      },
      "source": [
        "## 2. $\\mathrm{TD}_{n}$ - N-step Q-value estimation\n",
        "\n",
        "$N$-step TD ($\\mathrm{TD}_{n}$) was introduced long before neural network based RL. In regular TD, we supervise the Q-network with single-step reward summed with highest Q-value of the next state. In contrast to that, $\\mathrm{TD}_{n}$ accumulated rewards over $n$ steps and sums it with the highest Q-value of the state that occured after $n$ steps [(Sutton 1988)](http://incompleteideas.net/papers/sutton-88-with-erratum.pdf). Double DQN $\\mathrm{TD}_{n}$ loss is defined by:\n",
        "\n",
        "$$\n",
        "\\mathrm{TD}_{n}(s_i, a_i, s^{'}_{i+n}) = Q_{\\theta}~(s_i,a_i) - \\biggl(\\sum_{k=0}^{n-1} \\gamma^{k} ~ r_{(s_{i+k},a_{i+k},s_{i+k}^{'})} + \\gamma^{n} \\underset{a^{'}_{i+n} \\sim \\bar{Q}_{\\theta}}{\\mathrm{max}} ~ \\bar{Q}_{\\theta}~(s_{i+n}^{'},a_{i+n}^{'}) \\biggr)\n",
        "$$\n",
        "\n",
        "Implementing $\\mathrm{TD}_{n}$ requires changes to the ExperienceBuffer class. We will implement those changes using the **deque** module. This module will store $n$ of the most recent transitions, and will act as an intermediate between agent and buffers main storage. As compared to single step reward and $s_{i}^{'}$ stored by the simple ExperienceBuffer, the main storage of this upgraded buffer should store $n$ step rewards and $s_{i+n}^{'}$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f448764d",
      "metadata": {
        "id": "f448764d"
      },
      "source": [
        "### Task 2.1 Implement NStepBuffer\n",
        "Implement *get_nstep* method for **NStepBuffer** class (no other method of base class should be changed). The *get_nstep* method should process current memory and output a tuple of five:\n",
        "* state for which the $\\mathrm{TD}_{n}$ reward was computed,\n",
        "* action chosen in that step in processed trajectory,\n",
        "* $\\mathrm{TD}_{n}$ reward computed using *nstep* rewards,\n",
        "* state reached after *nstep* steps (possibly earlier if terminal state was encountered),\n",
        "* terminal flag, that notifies wheather trajectory has reached terminal state within *nstep* steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12560df7",
      "metadata": {
        "id": "12560df7"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "class NStepBuffer(ExperienceBuffer):\n",
        "    def __init__(self, args, nstep):\n",
        "        super(NStepBuffer, self).__init__(args)\n",
        "        self.memories = deque(maxlen=nstep)\n",
        "        self.nstep = nstep \n",
        "        \n",
        "    def add(self, state, action, reward, next_state, terminal):\n",
        "        terminal_ = 1 if terminal else 0 \n",
        "        memory = (state, action, reward, next_state, terminal_)\n",
        "        self.memories.append(memory)\n",
        "        if len(self.memories) >= self.nstep:\n",
        "            state, action, reward, next_state, terminal = self.get_nstep()\n",
        "            self.states[self.idx, :] = state\n",
        "            self.actions[self.idx, :] = action\n",
        "            self.rewards[self.idx, :] = reward\n",
        "            self.next_states[self.idx, :] = next_state\n",
        "            self.terminals[self.idx, :] = terminal\n",
        "            self.idx += 1\n",
        "            if self.idx == self.args.capacity:\n",
        "                self.full = True\n",
        "                self.idx = 0\n",
        "            \n",
        "    def get_nstep(self):\n",
        "        ####### TODO ########\n",
        "        \n",
        "        ####### END ########\n",
        "        return state, action, reward, next_state, terminal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d45601",
      "metadata": {
        "id": "34d45601"
      },
      "source": [
        "### Task 2.2: Implement and train N-step DQN \n",
        "Implement the *update* method for **NStepDQN** class (no other method of base class should be changed): "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d1cb4c6",
      "metadata": {
        "id": "0d1cb4c6"
      },
      "outputs": [],
      "source": [
        "class NStepDQN(DQN):\n",
        "    def __init__(self, args, nstep=3):\n",
        "        super(NStepDQN, self).__init__(args)\n",
        "        self.nstep = nstep\n",
        "        self.buffer = NStepBuffer(args, nstep)\n",
        "        \n",
        "    def update(self):\n",
        "        states, actions, rewards, next_states, terminals = self.buffer.sample()\n",
        "        ####### TODO ########\n",
        "        # q_targets = \n",
        "        ####### END ########\n",
        "        self.optimizer.zero_grad()\n",
        "        q_values = self.q_net(states).gather(1, actions)\n",
        "        loss = nn.functional.smooth_l1_loss(q_values, q_targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def reset(self):\n",
        "        super().reset()\n",
        "        self.buffer = NStepBuffer(self.args, self.nstep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e27e7900",
      "metadata": {
        "id": "e27e7900"
      },
      "outputs": [],
      "source": [
        "agent = NStepDQN(args)\n",
        "results_dqn2 = train_agent(args, agent)\n",
        "download_numpy(\"results_dqn2.npy\", results_dqn2)\n",
        "results_dqn2.mean(1)[-10:].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba25c410",
      "metadata": {
        "id": "ba25c410"
      },
      "source": [
        "## 3. DUELING DQN\n",
        "\n",
        "State-action advantage under policy $\\pi$ is given by:\n",
        "\n",
        "$$\n",
        "A^\\pi (s, a) = Q^\\pi (s, a) - V^\\pi (s)\n",
        "$$\n",
        "\n",
        "Where $A^\\pi (s, a)$ denotes state-action advantage, $Q^\\pi (s, a)$ denotes state-action Q-value and $V^\\pi (s)$ denotes state value. Advantage is a measure of how much better a particular action is than the state value. Given optimal policy it follows that $\\underset{a}{\\mathrm{max}}~Q^\\pi (s, a) = V^\\pi (s)$ and as such $A^\\pi (s, a) \\leq 0$ if $\\pi$ is optimal. We can use advantages to redefine Q-values:\n",
        "\n",
        "$$\n",
        "Q^\\pi (s, a) = V^\\pi (s) + A^\\pi (s, a)\n",
        "$$\n",
        "\n",
        "As such, we can use separate networks to predict $A^\\pi (s, a)$ and $V^\\pi (s)$ and retrieve Q-values using the equation above. This is exactly the idea behind the Dueling Q-network architecture [(Wang et al. 2015)](https://arxiv.org/pdf/1511.06581.pdf). Decoupling Q-values into values and advantages offers some optimization benefits:\n",
        "\n",
        "1. $V^\\pi (s)$ is independent of actions, as such the value network will have less parameters than a Q-network\n",
        "2. $A^\\pi (s, a)$ although action dependent, advantages oscillate around 0 and change slowly throughout the optimization\n",
        "\n",
        "Intuitively, the dueling Q-network can more efficiently learn which states are valuable, even when the actions available in those states do not affect the environment in a meaningful way. This can be particularly helpful in large or complex environments where it may not be possible to learn good action values for every state-action pair. Dueling DQN architecture uses joint feature layer and two separate heads to represent advantage and value streams (look at Figure 1. in [(Wang et al. 2015)](https://arxiv.org/pdf/1511.06581.pdf)). To further smoothen the optimization, Dueling DQN Q-value is calculated with the following:\n",
        "\n",
        "$$\n",
        "Q_\\theta (s, a) = V_\\theta (s) + \\bigl( A_\\theta (s, a) - \\sum_{a} \\frac{A_\\theta (s, a)}{N_a} \\bigr),\n",
        "$$\n",
        "\n",
        "where $N_a$ is the number of possible actions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6ac9783",
      "metadata": {
        "id": "d6ac9783"
      },
      "source": [
        "### Task 3.1 Implement and train DuelingQNetwork\n",
        "\n",
        "Implement the **DuelingQNetwork** class and its *forward* method (no other method of the base class should be changed):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89b64c39",
      "metadata": {
        "id": "89b64c39"
      },
      "outputs": [],
      "source": [
        "class DuelingQNetwork(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(DuelingQNetwork, self).__init__()\n",
        "        ####### TODO ########\n",
        "        # self.layers = nn.Sequential(...)\n",
        "        # self.advantage_head = \n",
        "        # self.value_head = \n",
        "        ####### END ########\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ####### TODO ########\n",
        "\n",
        "        ####### END ########\n",
        "    \n",
        "class DuelingDQN(DQN):\n",
        "    def __init__(self, args):\n",
        "        super(DuelingDQN, self).__init__(args)\n",
        "        self.q_net = DuelingQNetwork(args).to(args.device)\n",
        "        self.q_target = DuelingQNetwork(args).to(args.device)\n",
        "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
        "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=args.learning_rate, eps=1e-5)\n",
        "        \n",
        "    def reset(self):\n",
        "        super().reset()\n",
        "        self.q_net = DuelingQNetwork(self.args).to(self.args.device)\n",
        "        self.q_target = DuelingQNetwork(self.args).to(self.args.device)\n",
        "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
        "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=self.args.learning_rate, eps=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a641e57",
      "metadata": {
        "id": "3a641e57"
      },
      "outputs": [],
      "source": [
        "agent = DuelingDQN(args)\n",
        "results_dqn3 = train_agent(args, agent)\n",
        "download_numpy(\"results_dqn3\", results_dqn3)\n",
        "results_dqn3.mean(1)[-10:].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69b90bf3",
      "metadata": {
        "id": "69b90bf3"
      },
      "source": [
        "## 4. (Almost) Rainbow\n",
        "The final thing we are left with is to combine all the improvements into a single agent [(Hessel 2017)](https://arxiv.org/pdf/1710.02298.pdf). To this end, you have to implement three classes:\n",
        "\n",
        "1. **RainbowBuffer** - experience buffer that combines nstep returns and priority-based sampling\n",
        "2. **RainbowQNetwork** - Q-network that uses noisy linear layers in a dueling setup\n",
        "3. **RainbowDQN** - DQN that combines all of the covered techniques\n",
        "\n",
        "![fig1](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_9.14.13_PM_4fMCutg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c964c779",
      "metadata": {
        "id": "c964c779"
      },
      "source": [
        "### Task 4.1 Implement RainbowBuffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "372a2972",
      "metadata": {
        "id": "372a2972"
      },
      "outputs": [],
      "source": [
        "class RainbowBuffer(NStepBuffer):\n",
        "    def __init__(self, args, nstep):\n",
        "        super(RainbowBuffer, self).__init__(args, nstep)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "759b4524",
      "metadata": {
        "id": "759b4524"
      },
      "source": [
        "### Task 4.2 Implement RainbowQNetwork class "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9889f6ee",
      "metadata": {
        "id": "9889f6ee"
      },
      "outputs": [],
      "source": [
        "class RainbowQNetwork(DuelingQNetwork):\n",
        "    def __init__(self, args):\n",
        "        super(RainbowQNetwork, self).__init__(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12406eb1",
      "metadata": {
        "id": "12406eb1"
      },
      "source": [
        "### Task 4.3 Implement and train RainbowDQN agent "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ee6f39",
      "metadata": {
        "id": "f3ee6f39"
      },
      "outputs": [],
      "source": [
        "class RainbowDQN(DQN):\n",
        "    def __init__(self, args, nstep=3):\n",
        "        super(RainbowDQN, self).__init__(args)\n",
        "        self.buffer = RainbowBuffer(args, nstep)\n",
        "        self.nstep = nstep\n",
        "        self.q_net = RainbowQNetwork(args).to(args.device)\n",
        "        self.q_target = RainbowQNetwork(args).to(args.device)\n",
        "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.q_net.parameters(), lr=args.learning_rate, eps=1e-5\n",
        "        )\n",
        "\n",
        "    def update(self):\n",
        "        ####### TODO ########\n",
        "        pass\n",
        "        ####### END ########\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = RainbowBuffer(self.args, self.nstep)\n",
        "        self.q_net = RainbowQNetwork(self.args).to(self.args.device)\n",
        "        self.q_target = RainbowQNetwork(self.args).to(self.args.device)\n",
        "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.q_net.parameters(), lr=self.args.learning_rate, eps=1e-5\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb1c384",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feb1c384",
        "outputId": "f98f4e33-1169-4e68-f34e-d1f993035888"
      },
      "outputs": [],
      "source": [
        "agent = RainbowDQN(args)\n",
        "results_dqn4 = train_agent(args, agent)\n",
        "download_numpy(\"results_dqn4.npy\", results_dqn4)\n",
        "results_dqn4.mean(1)[-10:].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a39b8c3",
      "metadata": {
        "id": "5a39b8c3"
      },
      "source": [
        "# Plot collected results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27373aca",
      "metadata": {
        "id": "27373aca"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.arange(results_dqn.shape[0])*1000\n",
        "plt.plot(x, results_dqn.mean(1), label='DQN')\n",
        "plt.plot(x, results_dqn1.mean(1), label='Double DQN')\n",
        "plt.plot(x, results_dqn2.mean(1), label='NStep DQN')\n",
        "plt.plot(x, results_dqn3.mean(1), label='Dueling DQN')\n",
        "plt.plot(x, results_dqn4.mean(1), label='Almost Rainbow')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2639d5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.boxplot(\n",
        "    labels=[\"DQN\", \"Double DQN\", \"NStep DQN\", \"Dueling DQN\", \"Almost Rainbow\"],\n",
        "    x=[\n",
        "        results_dqn[-3:].flatten(),\n",
        "        results_dqn1[-3:].flatten(),\n",
        "        results_dqn2[-3:].flatten(),\n",
        "        results_dqn3[-3:].flatten(),\n",
        "        results_dqn4[-3:].flatten(),\n",
        "    ],\n",
        ")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "atp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "6809863f01cf54cb8cf26991fcf8425a337722d7d35212492765d6bf47d2da35"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
