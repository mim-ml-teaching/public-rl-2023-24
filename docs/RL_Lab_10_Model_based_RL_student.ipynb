{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://i.postimg.cc/TPR1n1rp/AI-Tech-PL-RGB.png' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Programu Operacyjnego Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://i.postimg.cc/Gpq2KRQz/logotypy-aitech.jpg'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "</center>"
      ],
      "metadata": {
        "id": "bKx6i_cNLQ9_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT5WLEFNHjeJ"
      },
      "source": [
        "# Lab 06: Model-based Reinforcement Learning\n",
        "\n",
        "In this lab, we will reproduce the _Learning Inside of a Dream_ experiment of the seminal [World Models](https://worldmodels.github.io/) paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AYglcYZix508"
      },
      "outputs": [],
      "source": [
        "#@title Mount your Google Drive\n",
        "\n",
        "#@markdown Your work will be stored in a folder called `rl_lab_2022` by default.\n",
        "\n",
        "#@markdown Run each section with Shift+Enter\n",
        "\n",
        "#@markdown Double-click on section headers to show code.\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "LAB_PATH = '/content/gdrive/MyDrive/rl_lab_2022/model_based_rl'\n",
        "if not os.path.exists(LAB_PATH):\n",
        "  %mkdir -p $LAB_PATH\n",
        "\n",
        "# cd into the lab directory\n",
        "%cd $LAB_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_LA7e0OHhgt"
      },
      "source": [
        "## 0. Collect data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-video"
      ],
      "metadata": {
        "id": "W5-4JvrWVCse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import random\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import skvideo.io\n",
        "import tensorflow as tf\n",
        "\n",
        "from base64 import b64encode\n",
        "from IPython.display import HTML\n",
        "\n",
        "def show_video(file_name):\n",
        "    mp4 = open(file_name,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    return HTML(\"\"\"\n",
        "    <video width=256 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url)\n",
        "\n",
        "ACTIONS = [[0, 0], # no-op\n",
        "           [0, 1], # right\n",
        "           [1, 0]] # left\n",
        "FRAMES_PER_ACTION = 2 # 4? 8? 12??\n",
        "OBS_SIZE = (64, 64)\n",
        "TOTAL_STEPS = 225_000 # 2_000_000"
      ],
      "metadata": {
        "id": "mOz5edHZSeOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Install VizDoom..._"
      ],
      "metadata": {
        "id": "s88feiFOmu3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get update\n",
        "\n",
        "# Install deps from\n",
        "# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n",
        "apt-get -qq -y install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n",
        "nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n",
        "libopenal-dev timidity libwildmidi-dev unzip\n",
        "\n",
        "# Boost libraries\n",
        "apt-get -qq -y install libboost-all-dev\n",
        "\n",
        "# Lua binding dependencies\n",
        "apt-get -qq -y install liblua5.1-dev"
      ],
      "metadata": {
        "id": "RMc1F1N1mtNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vizdoom"
      ],
      "metadata": {
        "id": "A7XfbhhZVESr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om_yNoXo0z7d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import vizdoom as vzd\n",
        "\n",
        "TAKE_OVER_CONFIG = os.path.join(vzd.scenarios_path, 'take_cover.cfg')\n",
        "\n",
        "def initialize_doom_game():\n",
        "    game = vzd.DoomGame()\n",
        "    game.load_config(TAKE_OVER_CONFIG)\n",
        "    game.set_window_visible(False)\n",
        "    game.set_mode(vzd.Mode.PLAYER)\n",
        "    game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
        "    game.init()\n",
        "\n",
        "    return game"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def preprocess(obs):\n",
        "    obs = tf.transpose(obs, (1, 2, 0)) # Move the channel dim. to the end\n",
        "    obs = obs[80:400, :, :]\n",
        "    obs = tf.image.resize(obs, OBS_SIZE, method='area')\n",
        "    return tf.cast(obs, tf.uint8)\n",
        "\n",
        "@tf.function\n",
        "def batch_preprocess(obs):\n",
        "    obs = tf.transpose(obs, (0, 2, 3, 1)) # Move the channel dim. to the end\n",
        "    obs = obs[:, 80:400, :, :]\n",
        "    obs = tf.image.resize(obs, OBS_SIZE, method='area')\n",
        "    return tf.cast(obs, tf.uint8)"
      ],
      "metadata": {
        "id": "E9qgQfxfmydl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3MsftoK9hyU"
      },
      "outputs": [],
      "source": [
        "game = initialize_doom_game()\n",
        "obs_list = []\n",
        "game.new_episode()\n",
        "while not game.is_episode_finished():\n",
        "    obs_list.append(preprocess(game.get_state().screen_buffer))\n",
        "    game.make_action(random.choice(ACTIONS), FRAMES_PER_ACTION)\n",
        "frames = tf.stack(obs_list)\n",
        "print('Return: ', game.get_total_reward())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34UhXCS_I8l8"
      },
      "outputs": [],
      "source": [
        "file_name = 'take_over.mp4'\n",
        "skvideo.io.vwrite(file_name, frames)\n",
        "show_video(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X58XuNAFJHsz"
      },
      "source": [
        "### Collect a new dataset (optional!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loLthxeCsghj"
      },
      "outputs": [],
      "source": [
        "obs_array = np.empty([TOTAL_STEPS, *OBS_SIZE, 3], dtype=np.uint8)\n",
        "act_array = np.empty([TOTAL_STEPS, 1], dtype=np.int)\n",
        "done_array = np.empty([TOTAL_STEPS, 1], dtype=np.int)\n",
        "\n",
        "repeat = 4\n",
        "done = False\n",
        "game = initialize_doom_game()\n",
        "iter_time = time.time()\n",
        "for i in range(TOTAL_STEPS):\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        steps_per_second = 1000 / (time.time() - iter_time)\n",
        "        print(f'Step {(i + 1)//1000}k/{TOTAL_STEPS//1000}k, Steps per second: {steps_per_second:.0f}')\n",
        "        iter_time = time.time()\n",
        "\n",
        "    if i % repeat == 0:\n",
        "        repeat = np.random.randint(1, (10 // FRAMES_PER_ACTION) + 1)\n",
        "        act = random.randint(0, 2)\n",
        "\n",
        "    obs = preprocess(game.get_state().screen_buffer)\n",
        "    game.make_action(ACTIONS[act], FRAMES_PER_ACTION)\n",
        "    done = game.is_episode_finished()\n",
        "\n",
        "    if done:\n",
        "        game.new_episode()\n",
        "\n",
        "    obs_array[i] = obs\n",
        "    act_array[i] = act\n",
        "    done_array[i] = done\n",
        "\n",
        "np.savez('doom_data_225k.npz', observations=obs_array, actions=act_array, dones=done_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZhaZJcLLSFD"
      },
      "source": [
        "## 1. Vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HUn6uuto10c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "BASE_DEPTH = 32\n",
        "OBS_SIZE = (64, 64)\n",
        "INPUT_SHAPE = (*OBS_SIZE, 3)\n",
        "KL_TOLERANCE = 0.5\n",
        "LATENT_SIZE = 64\n",
        "\n",
        "LOGS_DIR = 'logs'\n",
        "VISION_IMAGES_DIR = os.path.join(LOGS_DIR, 'images/vision')\n",
        "VISION_CKPT_PATH = os.path.join(LOGS_DIR, 'best_vision')\n",
        "VISION_LOGS_PATH = os.path.join(LOGS_DIR, 'train_vision.csv')\n",
        "VISION_WEIGHTS_PATH = os.path.join(LOGS_DIR, 'best_vision.h5')\n",
        "\n",
        "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
        "  print('WARNING: GPU device not found.')\n",
        "else:\n",
        "  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kbs-xx-xJuW"
      },
      "outputs": [],
      "source": [
        "os.makedirs(LOGS_DIR, exist_ok=True)\n",
        "os.makedirs(VISION_IMAGES_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSnshe17Lawe"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIJE-yOspEg0"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(f'doom_data_225k.npz'):\n",
        "    raise ValueError('Collect the data first!')\n",
        "\n",
        "with np.load('doom_data_225k.npz') as data:\n",
        "    obs_array, act_array, done_array = data.values()\n",
        "\n",
        "TOTAL_STEPS = obs_array.shape[0]\n",
        "print('Number of samples: ', obs_array.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vision_preprocess(sample):\n",
        "    image = tf.cast(sample, tf.float32) / 255.  # Scale to unit interval.\n",
        "    return image, image"
      ],
      "metadata": {
        "id": "Fca0u1bvE2t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcvnT84SgYOy"
      },
      "outputs": [],
      "source": [
        "train_dataset = (tf.data.Dataset.from_tensor_slices((obs_array[:200000]))\n",
        "                 .shuffle(int(1e4))\n",
        "                 .batch(BATCH_SIZE)\n",
        "                 .map(vision_preprocess)\n",
        "                 .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices((obs_array[200000:]))\n",
        "                .batch(BATCH_SIZE)\n",
        "                .map(vision_preprocess)\n",
        "                .prefetch(tf.data.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLvgxFKVanG6"
      },
      "outputs": [],
      "source": [
        "def plot_samples(epoch, logs, vae, samples_iter, title, prefix):\n",
        "    samples = next(samples_iter)[0].numpy()\n",
        "    samples = samples[np.random.randint(0, samples.shape[0], size=12)]\n",
        "    x_pred = vae.predict(samples)\n",
        "\n",
        "    nrows, ncols = 2, samples.shape[0]\n",
        "    dx, dy = 1, 1\n",
        "    figsize = plt.figaspect(float(dy * nrows) / float(dx * ncols))\n",
        "\n",
        "    imgs = np.empty_like(np.concatenate((samples, x_pred)))\n",
        "    imgs[:ncols] = samples\n",
        "    imgs[ncols:] = x_pred\n",
        "\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(imgs[i], interpolation='none')\n",
        "        ax.set(xticks=[], yticks=[])\n",
        "\n",
        "    fig.suptitle(f'{title} [Ground Truth over Predicted]',\n",
        "                 fontsize=16)\n",
        "    plt.savefig(os.path.join(VISION_IMAGES_DIR, f'{prefix}_epoch_{epoch}'))\n",
        "    plt.close(fig)\n",
        "\n",
        "class PlotSamplesCallable:\n",
        "    def __init__(self, vae, samples_iter, title, prefix):\n",
        "        self.vae = vae\n",
        "        self.samples_iter = samples_iter\n",
        "        self.title = title\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def __call__(self, epoch, logs):\n",
        "        plot_samples(epoch, logs, self.vae, self.samples_iter, self.title, self.prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Based on this tutorial (https://www.tensorflow.org/tutorials/generative/cvae) fill in the gaps in the VAE code."
      ],
      "metadata": {
        "id": "2SwvPNa3Jmcn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPmruFHFuDOH"
      },
      "outputs": [],
      "source": [
        "encoder_input = tfk.Input(shape=INPUT_SHAPE)\n",
        "\n",
        "encoder_body = tfk.Sequential([\n",
        "    tfkl.Lambda(lambda x: x - 0.5),\n",
        "    ...\n",
        "    tfkl.Flatten()\n",
        "])(encoder_input)\n",
        "\n",
        "mu = ...\n",
        "logvar = ...\n",
        "sigma = ...\n",
        "z = ...\n",
        "\n",
        "encoder = tf.keras.Model(inputs=encoder_input,\n",
        "                         outputs=[z, mu, logvar])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKyFAQy5Wsnv"
      },
      "outputs": [],
      "source": [
        "decoder_input = tfk.Input(shape=[LATENT_SIZE])\n",
        "\n",
        "decoder_body = tfk.Sequential([\n",
        "    tfkl.Dense(32 * BASE_DEPTH, activation=tf.nn.relu),\n",
        "    tfkl.Reshape([1, 1, 32 * BASE_DEPTH]),\n",
        "    ...\n",
        "])(decoder_input)\n",
        "\n",
        "decoder = tf.keras.Model(inputs=decoder_input,\n",
        "                         outputs=decoder_body)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDpUNklsJOvW"
      },
      "outputs": [],
      "source": [
        "def mse_loss(x_true, x_pred):\n",
        "    # Reconstruction loss\n",
        "    # NOTE: Shall be a logistic loss (binary crossentropy),\n",
        "    #       but MSE was used in the official implementation.\n",
        "    r_loss = ...\n",
        "    r_loss = tf.reduce_mean(r_loss)\n",
        "\n",
        "    return r_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL1pswursCtO"
      },
      "outputs": [],
      "source": [
        "# (Augmented) KL loss\n",
        "kl_loss = ...\n",
        "kl_loss = tf.maximum(kl_loss, KL_TOLERANCE * LATENT_SIZE)\n",
        "kl_loss = tf.reduce_mean(kl_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bi9nrZ_DJQj8"
      },
      "outputs": [],
      "source": [
        "vae = tfk.Model(inputs=encoder.inputs,\n",
        "                outputs=decoder(encoder.outputs[0]))\n",
        "vae.add_loss(kl_loss) # Add the regularization loss\n",
        "vae.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-4),\n",
        "            loss=mse_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ID1oYxH9Jgtg"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=25),\n",
        "    tf.keras.callbacks.ModelCheckpoint(VISION_CKPT_PATH,\n",
        "                                       verbose=1,\n",
        "                                       save_best_only=True),\n",
        "    tf.keras.callbacks.CSVLogger(VISION_LOGS_PATH,\n",
        "                                 append=True)\n",
        "]\n",
        "\n",
        "callbacks.append(tf.keras.callbacks.LambdaCallback(\n",
        "    on_epoch_end=PlotSamplesCallable(vae=vae,\n",
        "                                     samples_iter=iter(train_dataset),\n",
        "                                     title='Train examples',\n",
        "                                     prefix='train')\n",
        "    ))\n",
        "\n",
        "callbacks.append(tf.keras.callbacks.LambdaCallback(\n",
        "    on_epoch_end=PlotSamplesCallable(vae=vae,\n",
        "                                     samples_iter=iter(test_dataset),\n",
        "                                     title='Test examples',\n",
        "                                     prefix='test')\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwIiCzkDe0d8"
      },
      "outputs": [],
      "source": [
        "history = vae.fit(train_dataset,\n",
        "                  epochs=224444444,\n",
        "                  initial_epoch=135,\n",
        "                  validation_data=test_dataset,\n",
        "                  callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54NccvQImsii"
      },
      "outputs": [],
      "source": [
        "vae.save_weights(VISION_WEIGHTS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqaSX_fT907w"
      },
      "source": [
        "<h3><center>...or...</center></h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LY6iLo7y90jr"
      },
      "outputs": [],
      "source": [
        "# vae = tfk.models.load_model(VISION_CKPT_PATH, custom_objects={'mse_loss': mse_loss})\n",
        "vae.load_weights(VISION_WEIGHTS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfSopggCn8Ii"
      },
      "source": [
        "## 2. Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AnpIUEObr4i"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "ACTIONS_NUM = 3\n",
        "BATCH_SIZE = 100\n",
        "DONE_WEIGHT = 10. # Factor of importance for done = 1. (rare case for loss).\n",
        "HIDDEN_DIM = 512\n",
        "MASK_VALUE = 0.0\n",
        "NUM_GAUSSIANS = 5\n",
        "TEMPERATURE = 1.15\n",
        "TF_LOG_SQRT_TWO_PI = tf.math.log(tf.math.sqrt(2 * np.pi))\n",
        "\n",
        "MODEL_CKPT_PATH = os.path.join(LOGS_DIR, 'best_model')\n",
        "MODEL_IMAGES_DIR = os.path.join(LOGS_DIR, 'images/model')\n",
        "MODEL_LOGS_PATH = os.path.join(LOGS_DIR, 'train_model.csv')\n",
        "MODEL_WEIGHTS_PATH = os.path.join(LOGS_DIR, 'best_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwuFCClsv8Xd"
      },
      "outputs": [],
      "source": [
        "os.makedirs(MODEL_IMAGES_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWJXVdG6nw-R"
      },
      "outputs": [],
      "source": [
        "def plot_imgs(x_true, x_pred):\n",
        "    nrows, ncols = 2, x_true.shape[0]\n",
        "    dx, dy = 1, 1\n",
        "    figsize = plt.figaspect(float(dy * nrows) / float(dx * ncols)) * 2\n",
        "\n",
        "    imgs = np.empty_like(np.concatenate((x_true, x_pred)))\n",
        "    imgs[:ncols] = x_true\n",
        "    imgs[ncols:] = x_pred\n",
        "\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(imgs[i], interpolation='none')\n",
        "        ax.set(xticks=[], yticks=[])\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the 10k episodes dataset"
      ],
      "metadata": {
        "id": "LR3oSQ8nAOtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOTAL_STEPS_10K = 2_000_000\n",
        "OBS_BATCH_SIZE = 1000\n",
        "\n",
        "latent_array = np.empty([TOTAL_STEPS_10K, LATENT_SIZE, 2], dtype=np.float32)\n",
        "obs_batch = np.empty([OBS_BATCH_SIZE, *OBS_SIZE, 3], dtype=np.uint8)\n",
        "act_array = np.empty([TOTAL_STEPS_10K, 1], dtype=np.int)\n",
        "done_array = np.empty([TOTAL_STEPS_10K, 1], dtype=np.int)\n",
        "\n",
        "done = False\n",
        "game = initialize_doom_game()\n",
        "iter_time = time.time()\n",
        "for itr in range(TOTAL_STEPS_10K // OBS_BATCH_SIZE):\n",
        "    # Collect\n",
        "    repeat = 0\n",
        "    for step in range(OBS_BATCH_SIZE):\n",
        "        if repeat == 0:\n",
        "            repeat = np.random.randint(1, (10 // FRAMES_PER_ACTION) + 1)\n",
        "            act = random.randint(0, 2)\n",
        "\n",
        "        obs = preprocess(game.get_state().screen_buffer)\n",
        "        game.make_action(ACTIONS[act], FRAMES_PER_ACTION)\n",
        "        done = game.is_episode_finished()\n",
        "\n",
        "        if done:\n",
        "            game.new_episode()\n",
        "\n",
        "        obs_batch[step] = obs\n",
        "        act_array[itr * OBS_BATCH_SIZE + step] = act\n",
        "        done_array[itr * OBS_BATCH_SIZE + step] = done\n",
        "\n",
        "        repeat -= 1\n",
        "\n",
        "    # Encode\n",
        "    idx_start = int(itr * OBS_BATCH_SIZE)\n",
        "    idx_end = int((itr + 1) * OBS_BATCH_SIZE)\n",
        "\n",
        "    z_pred = encoder.predict(vision_preprocess(obs_batch)[0])\n",
        "\n",
        "    latent_array[idx_start:idx_end, :, 0] = z_pred[1]\n",
        "    latent_array[idx_start:idx_end, :, 1] = z_pred[2]\n",
        "\n",
        "    # Log\n",
        "    steps_per_second = OBS_BATCH_SIZE / (time.time() - iter_time)\n",
        "    iter_time = time.time()\n",
        "\n",
        "    steps_left = TOTAL_STEPS_10K - ((itr + 1) * OBS_BATCH_SIZE)\n",
        "    print(f'Step {((itr + 1) * OBS_BATCH_SIZE)//1000}k/{TOTAL_STEPS_10K//1000}k, steps/sec: {steps_per_second:.0f}, ETA: {(steps_left / steps_per_second)//60:.0f} mins')\n",
        ""
      ],
      "metadata": {
        "id": "CqopUTQ_AVQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdTeJ-O37Atx"
      },
      "source": [
        "### Encode the dataset..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scbsaPlQVS_1"
      },
      "outputs": [],
      "source": [
        "latent_array = np.empty([TOTAL_STEPS, LATENT_SIZE, 2], dtype=np.float32)\n",
        "\n",
        "iter_time = time.time()\n",
        "for i in range(TOTAL_STEPS // 1000):\n",
        "    idx_start = int(i * 1000)\n",
        "    idx_end = int((i + 1) * 1000)\n",
        "\n",
        "    obs_batch = obs_array[idx_start:idx_end, ...]\n",
        "    obs_batch = vision_preprocess(obs_batch)[0]\n",
        "\n",
        "    z_pred = encoder.predict(obs_batch)\n",
        "    latent_array[idx_start:idx_end, :, 0] = z_pred[1]\n",
        "    latent_array[idx_start:idx_end, :, 1] = z_pred[2]\n",
        "\n",
        "    print(f'Step {(i + 1):3d}/{TOTAL_STEPS // 1000}, ETA: {(time.time() - iter_time) * (TOTAL_STEPS // 1000 - i):.0f}s')\n",
        "    iter_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noKUS55QLR2N"
      },
      "outputs": [],
      "source": [
        "# Calculate episode boarders\n",
        "cumsum = np.cumsum(np.ones_like(done_array))[:, None]\n",
        "ep_borders = [0, *cumsum[np.nonzero(done_array)]]\n",
        "\n",
        "latent_list = []\n",
        "action_list = []\n",
        "done_list = []\n",
        "for idx_start, idx_end in zip(ep_borders[:-1], ep_borders[1:]):\n",
        "    latent_list.append(\n",
        "        latent_array[idx_start:idx_end])\n",
        "    action_list.append(\n",
        "        act_array[idx_start:idx_end])\n",
        "    done_list.append(\n",
        "        done_array[idx_start:idx_end])\n",
        "\n",
        "with open('doom_data_2M.pkl', 'wb') as file:\n",
        "    pickle.dump([latent_list, action_list, done_list], file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jC_iUMQcqaf"
      },
      "source": [
        "### ...or load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUqOZr7UM1bs"
      },
      "outputs": [],
      "source": [
        "with open('doom_data_2M.pkl', 'rb') as file:\n",
        "    latent_list, action_list, done_list = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBlPV2dI10pm"
      },
      "outputs": [],
      "source": [
        "latent_ragged = tf.ragged.constant(latent_list)\n",
        "action_ragged = tf.ragged.constant(action_list)\n",
        "done_ragged = tf.ragged.constant(done_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_ragged.shape"
      ],
      "metadata": {
        "id": "uuriGrHPlvuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEIoDZjnTq-3"
      },
      "outputs": [],
      "source": [
        "def memory_preprocess(rlatent, raction, rdone):\n",
        "    latent, action, done = rlatent.to_tensor(), raction.to_tensor(), rdone.to_tensor()\n",
        "\n",
        "    mu, logvar = latent[..., 0], latent[..., 1]\n",
        "    sigma = tf.exp(logvar / 2.0)\n",
        "    z = mu + sigma * tf.random.normal(tf.shape(mu))\n",
        "\n",
        "    action = tf.one_hot(tf.squeeze(action, axis=1), depth=ACTIONS_NUM)\n",
        "\n",
        "    done = tf.cast(done, tf.float32) * 2. - 1.\n",
        "\n",
        "    return (z[:-1], action[:-1]), (z[1:], done[1:])\n",
        "\n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices((latent_ragged[:10000], action_ragged[:10000], done_ragged[:10000]))\n",
        "                 .shuffle(10000)\n",
        "                 .map(memory_preprocess)\n",
        "                 .padded_batch(BATCH_SIZE, padding_values=MASK_VALUE)\n",
        "                 .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices((latent_ragged[10000:], action_ragged[10000:], done_ragged[10000:]))\n",
        "                .map(memory_preprocess)\n",
        "                .padded_batch(BATCH_SIZE, padding_values=MASK_VALUE)\n",
        "                .prefetch(tf.data.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOuorqTR7TlV"
      },
      "outputs": [],
      "source": [
        "test_iter = iter(test_dataset)\n",
        "batch = next(test_iter)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch[0][0][3].shape"
      ],
      "metadata": {
        "id": "VsZS9-XQmLKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDclVTEWsjqx"
      },
      "outputs": [],
      "source": [
        "x_one = decoder.predict(batch[0][0][3][150:170]) # Step at the timestep `t`\n",
        "x_two = decoder.predict(batch[1][0][3][150:170]) # Step at the timestep `t+1`\n",
        "plot_imgs(x_one, x_two)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Base on the original code (https://github.com/hardmaru/WorldModelsExperiments/blob/fd982b9691a941b52c6addbde29bc801ca6202c8/doomrnn/doomrnn.py) implement the MDN-RNN losses."
      ],
      "metadata": {
        "id": "e_ZCLPnEKbaY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXaCDQhA7QFs"
      },
      "outputs": [],
      "source": [
        "obs, act = tfkl.Input(shape=[None, LATENT_SIZE]), tfkl.Input(shape=[None, ACTIONS_NUM])\n",
        "masked_input = tfkl.Masking(mask_value=MASK_VALUE)(tfkl.Concatenate(axis=-1)([obs, act]))\n",
        "\n",
        "lstm_output = tfkl.LSTM(HIDDEN_DIM, return_sequences=True)(masked_input)\n",
        "\n",
        "# Predict the mixture mu, logstd, logmix\n",
        "mdn_output = tfkl.TimeDistributed(tfkl.Dense(LATENT_SIZE * NUM_GAUSSIANS * 3))(lstm_output)\n",
        "# Predict the done (log unnormalized) probability\n",
        "done_output = tfkl.TimeDistributed(tfkl.Dense(1))(lstm_output)\n",
        "\n",
        "def compute_mask(tensor):\n",
        "    return tf.reduce_any(tf.math.not_equal(tensor, MASK_VALUE),\n",
        "                         axis=-1,\n",
        "                         keepdims=True)\n",
        "\n",
        "def unstack_mdn_coef(output):\n",
        "    batch_size = tf.shape(output)[0]\n",
        "    shape = [batch_size, -1, LATENT_SIZE, NUM_GAUSSIANS, 3]\n",
        "    return tf.unstack(tf.reshape(output, shape), axis=-1)\n",
        "\n",
        "def mdn_sample(mdn_output):\n",
        "    mu, logstd, logitmix = unstack_mdn_coef(mdn_output)\n",
        "    sigma = tf.exp(logstd)\n",
        "\n",
        "    idxs = tf.random.categorical(\n",
        "        tf.reshape(logitmix, [-1, NUM_GAUSSIANS]) / TEMPERATURE, 1)\n",
        "    idxs = tf.reshape(idxs, [*tf.shape(mu)[:3], 1])\n",
        "\n",
        "    mu_sampled = tf.gather(mu, idxs, batch_dims=3)\n",
        "    sigma_sampled = tf.gather(sigma, idxs, batch_dims=3)\n",
        "\n",
        "    return mu_sampled + sigma_sampled * tf.random.normal(tf.shape(mu_sampled))\n",
        "\n",
        "def mdn_loss(z_true, mdn_output):\n",
        "    mask = tf.cast(compute_mask(z_true), dtype=tf.float32)\n",
        "    z_true = tf.expand_dims(z_true, axis=-1)\n",
        "\n",
        "    mu, logstd, logitmix = unstack_mdn_coef(mdn_output)\n",
        "    # Normalize the log mixing coefficient\n",
        "    logmix = ...\n",
        "    sigma = tf.exp(logstd)\n",
        "\n",
        "    lognormal = ...\n",
        "    logmixnormal = ...\n",
        "\n",
        "    # You sum here because it's a mixture of Gaussian and not joint probabil\n",
        "    logprob = ...\n",
        "    loss = -tf.reduce_sum(logprob, axis=-1, keepdims=True)\n",
        "\n",
        "    return tf.reduce_sum(loss * mask) / tf.reduce_sum(mask)\n",
        "\n",
        "def done_loss(done_true, done_pred):\n",
        "    mask = tf.cast(compute_mask(done_true), dtype=tf.float32)\n",
        "    done_label = ((done_true + 1.) / 2.) * mask\n",
        "    weight = mask + done_label * (DONE_WEIGHT - 1.) # Weight pred. done = 1.\n",
        "\n",
        "    loss = ...\n",
        "\n",
        "    return tf.reduce_sum(loss * weight) / tf.reduce_sum(mask)\n",
        "\n",
        "mdn_rnn = tfk.models.Model(inputs=[obs, act], outputs=[mdn_output,  done_output])\n",
        "mdn_rnn.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
        "               loss=[mdn_loss, done_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHaP3lvJffNr"
      },
      "outputs": [],
      "source": [
        "def plot_samples(epoch, logs, mdn_rnn, decoder, batch, title, prefix):\n",
        "    idxs = np.random.randint(0, batch[0][0].shape[0], size=20)\n",
        "\n",
        "    mdn_output, _ = mdn_rnn(batch[0])\n",
        "    z_pred = mdn_sample(mdn_output).numpy()[idxs, 42, ...] # Get timestep = 42\n",
        "    z_true = batch[1][0].numpy()[idxs, 43, ...]\n",
        "\n",
        "    x_pred = decoder.predict(z_pred)\n",
        "    x_true = decoder.predict(z_true)\n",
        "\n",
        "    nrows, ncols = 2, x_true.shape[0]\n",
        "    dx, dy = 1, 1\n",
        "    figsize = plt.figaspect(float(dy * nrows) / float(dx * ncols))\n",
        "\n",
        "    imgs = np.empty_like(np.concatenate((x_true, x_pred)))\n",
        "    imgs[:ncols] = x_true\n",
        "    imgs[ncols:] = x_pred\n",
        "\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(imgs[i], interpolation='none')\n",
        "        ax.set(xticks=[], yticks=[])\n",
        "\n",
        "    fig.suptitle(f'{title} [Ground Truth over Predicted]',\n",
        "                 fontsize=16)\n",
        "    plt.savefig(os.path.join(MODEL_IMAGES_DIR, f'{prefix}_epoch_{epoch}'))\n",
        "    plt.close(fig)\n",
        "\n",
        "class PlotSamplesCallable:\n",
        "    def __init__(self, mdn_rnn, decoder, batch, title, prefix):\n",
        "        self.mdn_rnn = mdn_rnn\n",
        "        self.decoder = decoder\n",
        "        self.batch = batch\n",
        "        self.title = title\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def __call__(self, epoch, logs):\n",
        "        plot_samples(epoch, logs, self.mdn_rnn, self.decoder, self.batch, self.title, self.prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kii1YQsQCrpL"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=7),\n",
        "    tf.keras.callbacks.ModelCheckpoint(MODEL_CKPT_PATH,\n",
        "                                       verbose=1,\n",
        "                                       save_best_only=True),\n",
        "    tf.keras.callbacks.CSVLogger(MODEL_LOGS_PATH,\n",
        "                                 append=True)\n",
        "]\n",
        "\n",
        "callbacks.append(tf.keras.callbacks.LambdaCallback(\n",
        "    on_epoch_end=PlotSamplesCallable(mdn_rnn=mdn_rnn,\n",
        "                                     decoder=decoder,\n",
        "                                     batch=next(iter(train_dataset)),\n",
        "                                     title='Train memory examples',\n",
        "                                     prefix='train_memory')\n",
        "    ))\n",
        "\n",
        "callbacks.append(tf.keras.callbacks.LambdaCallback(\n",
        "    on_epoch_end=PlotSamplesCallable(mdn_rnn=mdn_rnn,\n",
        "                                     decoder=decoder,\n",
        "                                     batch=next(iter(test_dataset)),\n",
        "                                     title='Test memory examples',\n",
        "                                     prefix='test_memory')\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRavdNHEmQEv"
      },
      "outputs": [],
      "source": [
        "history = mdn_rnn.fit(train_dataset,\n",
        "                      epochs=224444444,\n",
        "                      # initial_epoch=17,\n",
        "                      validation_data=test_dataset,\n",
        "                      callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bpl8L4ovUR0"
      },
      "outputs": [],
      "source": [
        "mdn_rnn.save_weights(MODEL_WEIGHTS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkcLmWWZeClS"
      },
      "source": [
        "<h3><center>...or...</center></h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezZCsexzeB2D"
      },
      "outputs": [],
      "source": [
        "# mdn_rnn = tfk.models.load_model(MODEL_CKPT_PATH,\n",
        "#                                 custom_objects={'mdn_loss': mdn_loss,\n",
        "#                                                 'done_loss': done_loss})\n",
        "\n",
        "mdn_rnn.load_weights(MODEL_WEIGHTS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beOvHyCjoLLV"
      },
      "outputs": [],
      "source": [
        "mdn_output_, done_ = mdn_rnn.predict(batch[0])\n",
        "mdn_loss(batch[1][0], mdn_output_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74jspvQPWluU"
      },
      "outputs": [],
      "source": [
        "z_pred = mdn_sample(mdn_output_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bzm12Ad1e8Oo"
      },
      "outputs": [],
      "source": [
        "batch_idx = 8\n",
        "print('GT done: ', tf.where(batch[1][1][batch_idx] == 1.0)[0, 0])\n",
        "gt_zero_idxs = tf.where(done_[batch_idx] > 0.0)[:, 0]\n",
        "print('Pred. done: ', gt_zero_idxs)\n",
        "print('Pred. values: ', tf.gather(done_[batch_idx], gt_zero_idxs)[:, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2yq_6LwgJ0Q"
      },
      "outputs": [],
      "source": [
        "x_one = decoder.predict(batch[1][0][batch_idx][80:100]) # GT observations\n",
        "x_two = decoder.predict(z_pred[batch_idx][80:100]) # Predicted observations\n",
        "plot_imgs(x_one, x_two)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJAT9z24wv9g"
      },
      "source": [
        "### Create the world model!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WorldModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(WorldModel, self).__init__()\n",
        "        self.lstm = tfkl.LSTM(HIDDEN_DIM, return_state=True, stateful=True)\n",
        "        # Predict the mixture mu, logstd, logmix\n",
        "        self.mdn_head = tfkl.Dense(LATENT_SIZE * NUM_GAUSSIANS * 3)\n",
        "        # Predict the done (log unnormalized) probability\n",
        "        self.done_head = tfkl.Dense(1)\n",
        "\n",
        "        self._current_latent = None\n",
        "\n",
        "    def call(self, action, latent=None):\n",
        "        if latent is None:\n",
        "            latent = self._current_latent\n",
        "\n",
        "        concat_input = tf.concat([latent, action], axis=-1)\n",
        "        # Expand the time dimension\n",
        "        concat_input = tf.expand_dims(concat_input, axis=1)\n",
        "\n",
        "        lstm_output, lstm_hidden, _ = self.lstm(concat_input)\n",
        "        mdn_output = self.mdn_head(lstm_output)\n",
        "        done_output = self.done_head(lstm_output)\n",
        "\n",
        "        next_latent = mdn_sample(mdn_output)\n",
        "        if_done = done_output > 0.0\n",
        "\n",
        "        # Flatten the time dimension\n",
        "        next_latent = tf.reshape(next_latent, shape=[-1, LATENT_SIZE])\n",
        "        if_done = tf.reshape(if_done, shape=[-1, 1])\n",
        "\n",
        "        self._current_latent = next_latent\n",
        "        # Add the cell state too?\n",
        "        return (next_latent, lstm_hidden, if_done)\n",
        "\n",
        "    def reset(self, init_latent=None):\n",
        "        self.lstm.reset_states()\n",
        "        self._current_latent = init_latent"
      ],
      "metadata": {
        "id": "Qj4fN5yKy535"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "world_model = WorldModel()\n",
        "# Initialize the variables\n",
        "world_model(action=tf.zeros([1, ACTIONS_NUM]), latent=tf.zeros([1, LATENT_SIZE]))\n",
        "# Load the trained MDN-RNN weights\n",
        "world_model.load_weights(MODEL_WEIGHTS_PATH)"
      ],
      "metadata": {
        "id": "TLumuHInuExd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Meh_uZ3dzStG"
      },
      "outputs": [],
      "source": [
        "repeat = 4\n",
        "rollout_length = 400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCYD16px4t2x"
      },
      "outputs": [],
      "source": [
        "z_array = np.empty([rollout_length, LATENT_SIZE], dtype=np.float32)\n",
        "d_array = np.empty([rollout_length, 1], dtype=np.float32)\n",
        "\n",
        "z_array[0] = batch[0][0][0, 0, :]\n",
        "d_array[0] = np.NaN\n",
        "\n",
        "world_model.reset(z_array[None, 0])\n",
        "for i in range(rollout_length - 1):\n",
        "    if i % repeat == 0:\n",
        "        repeat = np.random.randint(1, (10 // FRAMES_PER_ACTION) + 1)\n",
        "        action = tf.one_hot(random.randint(0, 2), depth=ACTIONS_NUM)\n",
        "\n",
        "    next_z, _, done = world_model(action[None])\n",
        "\n",
        "    z_array[i + 1] = next_z[0]\n",
        "    d_array[i + 1] = done[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.where(d_array > 0.0)[:, 0])"
      ],
      "metadata": {
        "id": "LilW-ovbuuDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = np.empty([rollout_length, 64, 64, 3], dtype=np.uint8)\n",
        "for i, z in enumerate(z_array):\n",
        "    frames[i] = (decoder.predict(tf.expand_dims(z, axis=0))[0] * 255.).astype(np.uint8)"
      ],
      "metadata": {
        "id": "B75hyUisWUk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show imagined game!\n",
        "\n",
        "Note that:\n",
        "1. The VAE neural network learnt to **render** the game state!\n",
        "2. The RNN neural network learnt to **simulate** the game mechanics!\n",
        "\n",
        "Stop for a second and think about it. Programmers code this stuff by hand. The game engine usually operates on some discrete state and does discontinuous operations on this state. Here the neural networks learnt from random interactions with this game engine how to approximate it with linear algebra only!"
      ],
      "metadata": {
        "id": "XoisRBCWHhyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 'dream_take_over.mp4'\n",
        "skvideo.io.vwrite(file_name, frames)\n",
        "show_video(file_name)"
      ],
      "metadata": {
        "id": "AjOQwzskSni-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u81lXZSylFmH"
      },
      "source": [
        "## 3. Controller"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cma"
      ],
      "metadata": {
        "id": "CISoemAcM1Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cma\n",
        "\n",
        "MAX_STEPS = 500\n",
        "NUM_EPISODES = 5\n",
        "POPULATION_SIZE = 128\n",
        "STD_DEV_INIT = 0.1\n",
        "WEIGHT_DECAY = 0.001\n",
        "\n",
        "@tf.function\n",
        "def preprocess(obs):\n",
        "    obs = tf.transpose(obs, (1, 2, 0)) # Move the channel dim. to the end\n",
        "    obs = obs[80:400, :, :]\n",
        "    obs = tf.image.resize(obs, OBS_SIZE, method='area')\n",
        "    return tf.cast(obs, tf.float32) / 255."
      ],
      "metadata": {
        "id": "E7e__xWrNB0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearModel():\n",
        "    def __init__(self, num_agents=POPULATION_SIZE):\n",
        "        self.kernel = np.zeros([num_agents, LATENT_SIZE + HIDDEN_DIM, ACTIONS_NUM])\n",
        "        self.bias = np.zeros([num_agents, ACTIONS_NUM])\n",
        "\n",
        "    def __call__(self, latent, hidden):\n",
        "        concat_input = np.concatenate([latent, hidden], axis=-1)\n",
        "        logits = np.sum((concat_input[..., np.newaxis] * self.kernel), axis=1) + self.bias\n",
        "        return np.argmax(logits, axis=-1)\n",
        "\n",
        "    def set_parameters(self, params):\n",
        "        copy_params = np.copy(params)\n",
        "        self.bias = copy_params[:, :ACTIONS_NUM]\n",
        "        self.kernel = copy_params[:, ACTIONS_NUM:].reshape(-1, LATENT_SIZE + HIDDEN_DIM, ACTIONS_NUM)\n",
        "\n",
        "    @property\n",
        "    def num_parameters(self):\n",
        "        return (LATENT_SIZE + HIDDEN_DIM) * ACTIONS_NUM + ACTIONS_NUM"
      ],
      "metadata": {
        "id": "T_nXAjfRQOhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "world_model = WorldModel()\n",
        "world_model(action=tf.zeros([POPULATION_SIZE, ACTIONS_NUM]), latent=tf.zeros([POPULATION_SIZE, LATENT_SIZE]))\n",
        "world_model.load_weights(MODEL_WEIGHTS_PATH)\n",
        "\n",
        "controller = LinearModel()\n",
        "\n",
        "es = cma.CMAEvolutionStrategy(controller.num_parameters * [0.], STD_DEV_INIT, {'popsize': POPULATION_SIZE})"
      ],
      "metadata": {
        "id": "hfVfPOI5ofnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INIT_MU = np.array([latent[0, :, 0] for latent in latent_list[:POPULATION_SIZE]])\n",
        "INIT_SIGMA = np.array([np.exp(latent[0, :, 1] / 2.0) for latent in latent_list[:POPULATION_SIZE]])"
      ],
      "metadata": {
        "id": "UBcsQCNQFLh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Optimize the controller using the methods `stop`, `ask`, and `tell`. Documentation: https://cma-es.github.io/apidocs-pycma/cma.evolution_strategy.CMAEvolutionStrategy.html"
      ],
      "metadata": {
        "id": "12AHZbJnQuT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while ...:\n",
        "    # Sample\n",
        "    solutions = np.array(...)\n",
        "\n",
        "    # Evaluate\n",
        "    dones = []\n",
        "    returns = np.zeros([POPULATION_SIZE])\n",
        "    controller.set_parameters(solutions)\n",
        "    for ep_idx in range(NUM_EPISODES):\n",
        "        latent = INIT_MU + INIT_SIGMA * np.random.randn(*INIT_MU.shape)\n",
        "        hidden = np.zeros([POPULATION_SIZE, HIDDEN_DIM])\n",
        "        world_model.reset(latent)\n",
        "        for _ in range(MAX_STEPS):\n",
        "            action = controller(latent, hidden)\n",
        "            latent, hidden, done = world_model(tf.one_hot(action, depth=ACTIONS_NUM), latent)\n",
        "            dones.append(done)\n",
        "        dones.append(np.ones_like(dones[0]))\n",
        "        done_array = np.squeeze(np.array(dones), axis=-1).transpose(1, 0)\n",
        "        indices = tf.where(done_array)\n",
        "        returns += tf.math.segment_min(indices[:, 1], indices[:, 0])\n",
        "    returns /= NUM_EPISODES\n",
        "\n",
        "    # Improve\n",
        "    if WEIGHT_DECAY > 0:\n",
        "        l2_decay = np.mean(solutions * solutions, axis=1)\n",
        "        returns -= WEIGHT_DECAY * l2_decay\n",
        "\n",
        "    # Convert minimizer to maximizer.\n",
        "    values = (-1 * returns).numpy().tolist()\n",
        "\n",
        "    # Improve!\n",
        "    ...\n",
        "\n",
        "    # Log\n",
        "    es.disp()"
      ],
      "metadata": {
        "id": "8hey8OSIo3P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('mean_controller.npy', es.result[5])\n",
        "np.save('best_controller.npy', es.best.x)"
      ],
      "metadata": {
        "id": "LIipq6IYLMod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><center>...or...</center></h3>"
      ],
      "metadata": {
        "id": "Zdnq2ROkbVuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_params = np.load('mean_controller.npy')\n",
        "best_params = np.load('best_controller.npy')\n",
        "rand_params = np.random.randn(*best_params.shape)"
      ],
      "metadata": {
        "id": "PWjAUpJipje2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "world_model = WorldModel()\n",
        "world_model(action=tf.zeros([1, ACTIONS_NUM]), latent=tf.zeros([1, LATENT_SIZE]))\n",
        "world_model.load_weights(MODEL_WEIGHTS_PATH)\n",
        "\n",
        "controller = LinearModel()\n",
        "\n",
        "game = initialize_doom_game()"
      ],
      "metadata": {
        "id": "ORyEBe7zx2lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_episodes = 42\n",
        "returns = []\n",
        "controller.set_parameters(best_params[None, ...])\n",
        "mean_time = 0.\n",
        "iter_time = time.time()\n",
        "for i in range(eval_episodes):\n",
        "    game.new_episode()\n",
        "    world_model.reset()\n",
        "    hidden = np.zeros([1, HIDDEN_DIM])\n",
        "    while not game.is_episode_finished():\n",
        "        # Get the current latent state\n",
        "        obs = preprocess(game.get_state().screen_buffer)\n",
        "        latent = encoder.predict(tf.expand_dims(obs, axis=0))[1] # mean\n",
        "\n",
        "        # Step the environment and the world model\n",
        "        action = controller(latent, hidden)\n",
        "        game.make_action(ACTIONS[int(action[0])], FRAMES_PER_ACTION)\n",
        "        _, hidden, _ = world_model(tf.one_hot(action, depth=ACTIONS_NUM), latent)\n",
        "    returns.append(game.get_total_reward())\n",
        "\n",
        "    mean_time = (mean_time * i + (time.time() - iter_time)) / (i + 1)\n",
        "    print(f'Step {(i + 1):2d}/{eval_episodes}, ETA: {mean_time * (eval_episodes - i):.0f}s')\n",
        "    iter_time = time.time()\n",
        "print('Avg. return: ', np.mean(returns),  '; Std. dev.: ', np.std(returns))"
      ],
      "metadata": {
        "id": "L6axq3WYqtP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Random agent: 299 +/-  101\n",
        "* Best agent: 493 +/- 318\n",
        "* Mean agent: 897 +/- 516"
      ],
      "metadata": {
        "id": "uBlwxPaH0LMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "controller.set_parameters(mean_params[None, ...])\n",
        "\n",
        "game.new_episode()\n",
        "world_model.reset()\n",
        "hidden = np.zeros([1, HIDDEN_DIM])\n",
        "obs_list = []\n",
        "while not game.is_episode_finished():\n",
        "    # Get the current latent state\n",
        "    obs = preprocess(game.get_state().screen_buffer)\n",
        "    obs_list.append(obs)\n",
        "    latent = encoder.predict(tf.expand_dims(obs, axis=0))[1] # mean\n",
        "\n",
        "    # Step the environment and the world model\n",
        "    action = controller(latent, hidden)\n",
        "    game.make_action(ACTIONS[int(action[0])], FRAMES_PER_ACTION)\n",
        "    _, hidden, _ = world_model(tf.one_hot(action, depth=ACTIONS_NUM), latent)\n",
        "frames = tf.cast(tf.stack(obs_list) * 255.0, tf.uint8)"
      ],
      "metadata": {
        "id": "Xh1nmcMlZAbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 'take_over.mp4'\n",
        "skvideo.io.vwrite(file_name, frames)\n",
        "show_video(file_name)"
      ],
      "metadata": {
        "id": "aX9oFG2kZd1M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}